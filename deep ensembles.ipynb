{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "import random\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/senqicao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import scipy.misc as scimisc\n",
    "safe_list = [0,2,5,6,8,12,13,14,15,16,17,18,19,21,26]  # other alphabets have characters which look like digits\n",
    "m = sio.loadmat(\"./data/data_background.mat\")\n",
    "\n",
    "squished_set = []\n",
    "for safe_number in safe_list:\n",
    "    for alphabet in m['images'][safe_number]:\n",
    "        for letters in alphabet:\n",
    "            for letter in letters:\n",
    "                for example in letter:\n",
    "                    squished_set.append(scimisc.imresize(1 - example[0], (28,28)).reshape(1, 28*28))\n",
    "\n",
    "safe_images = np.concatenate(squished_set, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-ed7aceb9359c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      "Training X shape: (60000, 784)\n",
      "Testing X shape: (9000, 784)\n",
      "Validation X shape: (1000, 784)\n",
      "\n",
      "Training Y shape: (60000, 10)\n",
      "Testing Y shape: (9000, 10)\n",
      "Validation Y shape: (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./data/mnist', validation_size=0, one_hot=True)\n",
    "test_len = mnist.test.images.shape[0]\n",
    "validation_len = int(test_len * 0.1)\n",
    "num_label=10\n",
    "train_x = mnist.train.images\n",
    "test_x = mnist.test.images[validation_len : test_len, :]\n",
    "validation_x = mnist.test.images[ : validation_len, :]\n",
    "\n",
    "train_y = mnist.train.labels\n",
    "test_y = mnist.test.labels[validation_len : test_len]\n",
    "validation_y = mnist.test.labels[ : validation_len]\n",
    "\n",
    "print(\"\\nTraining X shape: \" + str(train_x.shape))\n",
    "print(\"Testing X shape: \" + str(test_x.shape))\n",
    "print(\"Validation X shape: \" + str(validation_x.shape))\n",
    "\n",
    "print(\"\\nTraining Y shape: \" + str(train_y.shape))\n",
    "print(\"Testing Y shape: \" + str(test_y.shape))\n",
    "print(\"Validation Y shape: \" + str(validation_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "h1=512\n",
    "a = 0.0005\n",
    "num_iter = 5000\n",
    "batch_size = 100\n",
    "networks = ['network1', 'network2', 'network3', 'network4', 'network5']\n",
    "#b=True\n",
    "def get_network(network_name):\n",
    "    x_image = tf.placeholder(tf.float32, shape = [None, 28, 28, 1],name = network_name + 'x' )\n",
    "    b = tf.placeholder_with_default(False,shape=(),name=network_name +'b')\n",
    "\n",
    "    with tf.variable_scope(network_name):\n",
    "        w_conv1 = tf.get_variable(network_name + 'w_conv1', [5,5, 1,32], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv2 = tf.get_variable(network_name + 'w_conv2', [5,5,32,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w0 = tf.get_variable(network_name + 'w_fc1', [7*7*64, h1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b0 = tf.get_variable(network_name + 'b_fc1', [1,h1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w1 = tf.get_variable(network_name + 'w_fc2', [512, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.get_variable(network_name + 'b_fc2', [1,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    # network\n",
    "    con1 = tf.nn.conv2d(x_image, w_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    h_conv1 = tf.nn.relu(tf.layers.batch_normalization(con1, training=b))\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    con2 = tf.nn.conv2d(h_pool1, w_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    h_conv2 = tf.nn.relu(tf.layers.batch_normalization(con2, training=b))\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    h_pool2_flat = tf.layers.flatten(h_pool2)\n",
    "\n",
    "    h = tf.nn.relu(tf.matmul(h_pool2_flat, w0) + b0)\n",
    "    logits = tf.matmul(h, w1) + b1\n",
    "    output = tf.nn.softmax(logits)\n",
    "\n",
    "    y_label = tf.placeholder(tf.float32, shape = [None, 10],name = network_name + 'y' )\n",
    "\n",
    "    loss = tf.reduce_mean(tf.div(tf.reduce_sum(tf.square(tf.subtract(output, y_label)), axis = 1), num_label), axis = 0) \n",
    "   \n",
    "    # Get trainable variables\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, network_name)\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, network_name)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_opt = tf.train.AdamOptimizer(a).minimize(loss, var_list = train_vars)\n",
    "    return x_image, y_label, output, loss, train_opt, train_vars,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-1ca6f1eae778>:23: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-1ca6f1eae778>:30: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-1ca6f1eae778>:38: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "output_list = []\n",
    "loss_list = []\n",
    "train_list = []\n",
    "train_var_list = []\n",
    "b_list=[]\n",
    "\n",
    "# Train each ensemble network\n",
    "for i in range(len(networks)):\n",
    "    x_image, y_label, output, loss, train_opt, train_vars, b = get_network(networks[i])\n",
    "\n",
    "    x_list.append(x_image)\n",
    "    y_list.append(y_label)\n",
    "    output_list.append(output)\n",
    "    loss_list.append(loss)\n",
    "    train_list.append(train_opt)\n",
    "    train_var_list.append(train_vars)\n",
    "    b_list.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Session\n",
    "saver = tf.train.Saver(max_to_keep = 200)\n",
    "#config = tf.ConfigProto()\n",
    "#sess = tf.InteractiveSession(config=config)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model_id=0\n",
    "#save_path = saver.restore(sess, \"./ensemble/classification_mnist33/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accs:  [0.0982 0.1565 0.1423 0.0974 0.1494]\n",
      "0 Final Testing Accuracy:  0.1448\n",
      "maxp_OOD: 0.0 0.16270147 0.004554042\n",
      "maxp_inD: 0.0 0.16355546 0.0052132704\n",
      "ent_OOD: 2.249514 0.011949382 ent_in: 2.2436507 0.012633627\n",
      "AUPR_p: 50.83\n",
      "AUROC_p: 53.63\n",
      "AUPR_entropy: 59.56\n",
      "AUROC_entropy: 63.56\n",
      "AUPR_MI: 42.86\n",
      "AUROC_MI: 37.05\n",
      "############################################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-88afdbec3953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_size = 10000\n",
    "\n",
    "num_iter = 60000\n",
    "batch_size = 100\n",
    "\n",
    "train_data_num = train_x.shape[0]\n",
    "test_data_num  = test_x.shape[0]\n",
    "img_size=28\n",
    "safe_images = np.reshape(safe_images,[9200,28,28,1])\n",
    "\n",
    "for iter in range(num_iter):\n",
    "    #test\n",
    "    if iter % 100 == 0:\n",
    "        \n",
    "        x_batch_test = np.reshape(mnist.test.images,[10000, 28, 28, 1])\n",
    "        y_batch_test = mnist.test.labels\n",
    "        \n",
    "        outputs=[]\n",
    "        #b=False\n",
    "        for i in range(len(networks)):\n",
    "            prob = sess.run(output_list[i], {x_list[i]: x_batch_test, y_list[i]: y_batch_test,b_list[i]:False})\n",
    "            outputs.append(prob)\n",
    "        \n",
    "        outputs=np.array(outputs) \n",
    "        \n",
    "        accs = np.mean(np.argmax(outputs,2)==np.argmax(y_batch_test,1),1)\n",
    "        print(\"accs: \",accs)\n",
    "        \n",
    "        softmax = np.mean(outputs,0)\n",
    "        maxp_in = np.max(softmax,1)\n",
    "        acc = np.mean(np.argmax(softmax,1) ==  np.argmax(y_batch_test,1))\n",
    "        print( iter, 'Final Testing Accuracy: ', acc)\n",
    "\n",
    "        ent_in = np.sum(-np.log(softmax+1e-11)*softmax,1)\n",
    "        Eent_in = np.mean(np.sum(-np.log(outputs+1e-11)*outputs,2),0)\n",
    "        MI_in = ent_in - Eent_in\n",
    "        \n",
    "        outputs_OOD=[]\n",
    "        for j in range(len(networks)):\n",
    "            prob_OOD = sess.run(output_list[j], {x_list[j]: safe_images})\n",
    "            outputs_OOD.append(prob_OOD)\n",
    "            \n",
    "        outputs_OOD=np.array(outputs_OOD) \n",
    "        softmax_OOD = np.mean(outputs_OOD,0)\n",
    "        maxp_OOD = np.max(softmax_OOD,1)\n",
    "        ent_OOD = np.sum(-np.log(softmax_OOD+1e-11)*softmax_OOD,1)\n",
    "        Eent_OOD = np.mean(np.sum(-np.log(outputs_OOD+1e-11)*outputs_OOD,2),0)\n",
    "        MI_OOD = ent_OOD - Eent_OOD\n",
    "        \n",
    "        print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "        print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "        print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "\n",
    "        safe, risky  = -np.reshape(maxp_in,[10000,1]), -np.reshape(maxp_OOD,[9200,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_p:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_p:', round(100*roc_auc_score(labels, examples), 2))\n",
    "\n",
    "        safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[9200,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_entropy:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_entropy:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        \n",
    "        safe, risky = np.reshape(MI_in,[10000,1]), np.reshape(MI_OOD,[9200,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_MI:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_MI:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"############################################\")\n",
    " \n",
    "    #train\n",
    "    \n",
    "    if iter < 1000:\n",
    "        a = 0.005\n",
    "    elif iter < 5000:\n",
    "        a = 0.001\n",
    "    elif iter < 20000:\n",
    "        a = 0.0005\n",
    "    elif iter < 30000:\n",
    "        a = 0.0001\n",
    "    elif iter < 40000:\n",
    "        a = 0.00005\n",
    "    else:\n",
    "        a = 0.00001\n",
    "    for k in range(len(networks)):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        x_batch = np.reshape(batch[0],[batch_size, 28, 28, 1])\n",
    "        y_batch = batch[1]\n",
    "        sess.run(train_list[k], {x_list[k]:x_batch,y_list[k]: y_batch,b_list[k]:True})\n",
    "        \n",
    "    if iter%500==0:\n",
    "        save_path = saver.save(sess, \"./ensemble/classification_mnist%s/model.ckpt\" % model_id)\n",
    "        model_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'network1x:0' shape=(?, 28, 28, 1) dtype=float32>,\n",
       " <tf.Tensor 'network2x:0' shape=(?, 28, 28, 1) dtype=float32>,\n",
       " <tf.Tensor 'network3x:0' shape=(?, 28, 28, 1) dtype=float32>,\n",
       " <tf.Tensor 'network4x:0' shape=(?, 28, 28, 1) dtype=float32>,\n",
       " <tf.Tensor 'network5x:0' shape=(?, 28, 28, 1) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Adam' type=NoOp>,\n",
       " <tf.Operation 'Adam_1' type=NoOp>,\n",
       " <tf.Operation 'Adam_2' type=NoOp>,\n",
       " <tf.Operation 'Adam_3' type=NoOp>,\n",
       " <tf.Operation 'Adam_4' type=NoOp>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
