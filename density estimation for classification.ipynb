{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-f49db6432b7a>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/senqicao/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Extracting ./data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./data/mnist', one_hot=True)\n",
    "fashion_mnist = input_data.read_data_sets('./data/fashion', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_train_OOD = fashion_mnist.train.images\n",
    "y_train_OOD = fashion_mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_OOD[1,:].reshape(28,28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[1,:].reshape(28,28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 0.309341907501 train accuracy 0.0 test accuracy 0.1032 0.123381995\n",
      "time: 1000 33.2180628777 train accuracy 0.0 test accuracy 0.5608 0.16092953\n",
      "time: 2000 62.0105650425 train accuracy 0.0 test accuracy 0.7477 0.2168682\n",
      "time: 3000 88.4125459194 train accuracy 0.0 test accuracy 0.8135 0.10049924\n",
      "time: 4000 117.719063997 train accuracy 1.0 test accuracy 0.8331 0.7945353\n",
      "time: 5000 148.908906937 train accuracy 1.0 test accuracy 0.8576 0.964233\n",
      "time: 6000 177.619235039 train accuracy 1.0 test accuracy 0.8638 0.7280372\n",
      "time: 7000 204.468651056 train accuracy 1.0 test accuracy 0.8689 0.649335\n",
      "time: 8000 231.798812866 train accuracy 1.0 test accuracy 0.8816 0.46726128\n",
      "time: 9000 259.427762985 train accuracy 1.0 test accuracy 0.8923 0.59959376\n",
      "time: 10000 286.157338858 train accuracy 0.0 test accuracy 0.894 0.075427406\n",
      "time: 11000 312.269670963 train accuracy 1.0 test accuracy 0.8887 0.7660739\n",
      "time: 12000 338.131321907 train accuracy 0.0 test accuracy 0.8947 0.1827956\n",
      "time: 13000 364.142416 train accuracy 1.0 test accuracy 0.904 0.9916996\n",
      "time: 14000 390.409551859 train accuracy 0.0 test accuracy 0.9039 0.16703956\n",
      "time: 15000 418.545964956 train accuracy 1.0 test accuracy 0.8985 0.99557424\n",
      "time: 16000 447.013738871 train accuracy 0.0 test accuracy 0.9091 0.272686\n",
      "time: 17000 475.666646957 train accuracy 1.0 test accuracy 0.9062 0.98460096\n",
      "time: 18000 501.687651873 train accuracy 1.0 test accuracy 0.904 0.87299126\n",
      "time: 19000 528.133545876 train accuracy 1.0 test accuracy 0.9084 0.99977785\n",
      "time: 20000 554.109598875 train accuracy 0.0 test accuracy 0.9112 0.18815288\n",
      "time: 21000 580.218598843 train accuracy 1.0 test accuracy 0.9072 0.9989243\n",
      "time: 22000 618.296241045 train accuracy 1.0 test accuracy 0.9114 0.8538765\n",
      "time: 23000 656.780966997 train accuracy 1.0 test accuracy 0.9184 0.91037565\n",
      "time: 24000 688.300354958 train accuracy 1.0 test accuracy 0.9158 0.94187194\n",
      "time: 25000 717.253890038 train accuracy 1.0 test accuracy 0.9213 0.9936841\n",
      "time: 26000 746.244430065 train accuracy 1.0 test accuracy 0.9205 0.9990484\n",
      "time: 27000 775.541050911 train accuracy 1.0 test accuracy 0.9178 0.9811132\n",
      "time: 28000 803.843127012 train accuracy 1.0 test accuracy 0.922 0.9910262\n",
      "time: 29000 832.346611977 train accuracy 0.0 test accuracy 0.9204 0.00036038336\n",
      "time: 30000 860.325785875 train accuracy 1.0 test accuracy 0.9242 0.9927408\n",
      "time: 31000 887.471356869 train accuracy 1.0 test accuracy 0.9205 0.9623627\n",
      "time: 32000 914.250762939 train accuracy 1.0 test accuracy 0.9237 0.99749374\n",
      "time: 33000 940.288414955 train accuracy 1.0 test accuracy 0.9283 0.99435776\n",
      "time: 34000 969.153010845 train accuracy 1.0 test accuracy 0.9274 0.96538806\n",
      "time: 35000 997.836364031 train accuracy 1.0 test accuracy 0.9258 0.99999344\n",
      "time: 36000 1026.34647989 train accuracy 1.0 test accuracy 0.9296 0.93669784\n",
      "time: 37000 1055.32242203 train accuracy 1.0 test accuracy 0.9277 0.9880896\n",
      "time: 38000 1084.67912006 train accuracy 1.0 test accuracy 0.9298 0.97886044\n",
      "time: 39000 1113.0396359 train accuracy 1.0 test accuracy 0.9282 0.98345226\n",
      "time: 40000 1141.71087098 train accuracy 1.0 test accuracy 0.9294 0.984666\n",
      "time: 41000 1170.50011587 train accuracy 1.0 test accuracy 0.929 0.9925787\n",
      "time: 42000 1197.01505589 train accuracy 1.0 test accuracy 0.9326 0.8676917\n",
      "time: 43000 1223.19433498 train accuracy 1.0 test accuracy 0.9304 0.97510797\n",
      "time: 44000 1251.53503299 train accuracy 1.0 test accuracy 0.9308 0.9933496\n",
      "time: 45000 1293.18944693 train accuracy 1.0 test accuracy 0.9312 0.9978058\n",
      "time: 46000 1323.17604589 train accuracy 1.0 test accuracy 0.9367 0.9978411\n",
      "time: 47000 1353.44588184 train accuracy 1.0 test accuracy 0.9333 0.99952316\n",
      "time: 48000 1382.78352094 train accuracy 1.0 test accuracy 0.9327 0.9984835\n",
      "time: 49000 1412.81744003 train accuracy 1.0 test accuracy 0.9357 0.9858509\n",
      "time: 50000 1447.52431202 train accuracy 1.0 test accuracy 0.9379 0.7701121\n",
      "time: 51000 1483.0673759 train accuracy 1.0 test accuracy 0.9347 0.95002633\n",
      "time: 52000 1517.80983186 train accuracy 1.0 test accuracy 0.9331 0.99918383\n",
      "time: 53000 1552.48492503 train accuracy 1.0 test accuracy 0.94 0.9978806\n",
      "time: 54000 1587.44679785 train accuracy 1.0 test accuracy 0.9348 0.97565496\n",
      "time: 55000 1622.58985901 train accuracy 1.0 test accuracy 0.9402 0.9953702\n",
      "time: 56000 1657.35645294 train accuracy 0.0 test accuracy 0.9399 0.17688943\n",
      "time: 57000 1691.36900306 train accuracy 1.0 test accuracy 0.9394 0.67135096\n",
      "time: 58000 1723.16988993 train accuracy 1.0 test accuracy 0.9397 0.9953491\n",
      "time: 59000 1753.97386384 train accuracy 1.0 test accuracy 0.9416 0.998877\n",
      "time: 60000 1783.65145802 train accuracy 1.0 test accuracy 0.9401 0.9559273\n",
      "time: 61000 1812.50893688 train accuracy 1.0 test accuracy 0.9406 0.99995375\n",
      "time: 62000 1842.99650502 train accuracy 1.0 test accuracy 0.9434 0.9942731\n",
      "time: 63000 1874.559026 train accuracy 1.0 test accuracy 0.9425 0.9834448\n",
      "time: 64000 1907.39886498 train accuracy 1.0 test accuracy 0.9442 0.9482158\n",
      "time: 65000 1943.43771386 train accuracy 1.0 test accuracy 0.942 0.9053261\n",
      "time: 66000 1973.74029493 train accuracy 1.0 test accuracy 0.9421 0.99786973\n",
      "time: 67000 1999.80275297 train accuracy 1.0 test accuracy 0.9404 0.9704269\n",
      "time: 68000 2025.93806601 train accuracy 1.0 test accuracy 0.9444 0.9631927\n",
      "time: 69000 2053.77033091 train accuracy 1.0 test accuracy 0.9434 0.9739629\n",
      "time: 70000 2086.70582604 train accuracy 1.0 test accuracy 0.9461 0.7912217\n",
      "time: 71000 2114.32179093 train accuracy 1.0 test accuracy 0.9449 0.99984324\n",
      "time: 72000 2144.02602792 train accuracy 1.0 test accuracy 0.9453 0.8873898\n",
      "time: 73000 2172.67389703 train accuracy 1.0 test accuracy 0.9434 0.9996051\n",
      "time: 74000 2199.91174603 train accuracy 1.0 test accuracy 0.9455 0.9969248\n",
      "time: 75000 2234.09872389 train accuracy 1.0 test accuracy 0.9482 0.99803644\n",
      "time: 76000 2268.37631893 train accuracy 1.0 test accuracy 0.9454 0.7604781\n",
      "time: 77000 2298.98209786 train accuracy 1.0 test accuracy 0.9467 0.9987004\n",
      "time: 78000 2326.90612793 train accuracy 1.0 test accuracy 0.9498 0.99783677\n",
      "time: 79000 2355.56759787 train accuracy 1.0 test accuracy 0.9461 0.95351595\n",
      "time: 80000 2384.756603 train accuracy 1.0 test accuracy 0.9503 0.81931704\n",
      "time: 81000 2412.68176985 train accuracy 1.0 test accuracy 0.9488 0.90457374\n",
      "time: 82000 2441.01074195 train accuracy 1.0 test accuracy 0.9499 0.9999591\n",
      "time: 83000 2468.48664999 train accuracy 1.0 test accuracy 0.9482 0.9984812\n",
      "time: 84000 2495.195153 train accuracy 1.0 test accuracy 0.9475 0.9946247\n",
      "time: 85000 2522.10909605 train accuracy 0.0 test accuracy 0.9521 0.15484662\n",
      "time: 86000 2549.50132895 train accuracy 1.0 test accuracy 0.9505 0.99582195\n",
      "time: 87000 2578.91889906 train accuracy 1.0 test accuracy 0.9532 0.99796957\n",
      "time: 88000 2605.72922993 train accuracy 1.0 test accuracy 0.9497 0.99954236\n",
      "time: 89000 2633.59779191 train accuracy 1.0 test accuracy 0.9516 0.987159\n",
      "time: 90000 2661.34420991 train accuracy 1.0 test accuracy 0.954 0.8472295\n",
      "time: 91000 2689.45706701 train accuracy 1.0 test accuracy 0.952 0.9971662\n",
      "time: 92000 2717.47665596 train accuracy 1.0 test accuracy 0.9526 0.5469355\n",
      "time: 93000 2744.99079585 train accuracy 1.0 test accuracy 0.9524 0.9983151\n",
      "time: 94000 2772.80668688 train accuracy 1.0 test accuracy 0.9529 0.88357407\n",
      "time: 95000 2800.06619787 train accuracy 1.0 test accuracy 0.952 0.88476396\n",
      "time: 96000 2829.71618199 train accuracy 1.0 test accuracy 0.9518 0.4670749\n",
      "time: 97000 2858.58138394 train accuracy 1.0 test accuracy 0.9499 0.9558298\n",
      "time: 98000 2889.4982419 train accuracy 1.0 test accuracy 0.9528 0.99938726\n",
      "time: 99000 2917.88881493 train accuracy 1.0 test accuracy 0.9523 0.7419512\n",
      "time: \n",
      "2948.69942498\n"
     ]
    }
   ],
   "source": [
    "def neural_network_test(x, W_0, b_0, W_1, b_1,W_2, b_2):\n",
    "    h = tf.nn.relu(tf.matmul(x, W_0) + b_0)\n",
    "    h = tf.nn.relu(tf.matmul(h, W_1) + b_1)\n",
    "    h = (tf.matmul(h, W_2) + b_2)\n",
    "    y = tf.nn.softmax(h)\n",
    "    return y\n",
    "\n",
    "M =1\n",
    "D = 28*28\n",
    "D2 = 10\n",
    "n_intergal_sample = 10\n",
    "sigma0=.3\n",
    "h1 = 300\n",
    "h2 = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "Y = tf.placeholder(tf.float32, [None,D2], name='Y')\n",
    "X = tf.placeholder(tf.float32, [None,D], name='X')\n",
    "\n",
    "\n",
    "mu_w0 = tf.get_variable(\"w0_mu\",[D,h1])\n",
    "mu_b0 = tf.get_variable(\"b0_mu\",[1,h1])\n",
    "mu_w1 = tf.get_variable(\"w1_mu\",[h1,h2])\n",
    "mu_b1 = tf.get_variable(\"b1_mu\",[1,h2])\n",
    "mu_w2 = tf.get_variable(\"w2_mu\",[h2,D2])\n",
    "mu_b2 = tf.get_variable(\"b2_mu\",[1,D2])\n",
    "\n",
    "r_w0 = tf.get_variable(\"w0_sigma\",[D,h1])\n",
    "r_b0 = tf.get_variable(\"b0_sigma\",[1,h1])\n",
    "r_w1 = tf.get_variable(\"w1_sigma\",[h1,h2])\n",
    "r_b1 = tf.get_variable(\"b1_sigma\",[1,h2])\n",
    "r_w2 = tf.get_variable(\"w2_sigma\",[h2,D2])\n",
    "r_b2 = tf.get_variable(\"b2_sigma\",[1,D2])\n",
    "\n",
    "sigma_w0 = r_w0**2\n",
    "sigma_b0 = r_b0**2\n",
    "sigma_w1 = r_w1**2\n",
    "sigma_b1 = r_b1**2\n",
    "sigma_w2 = r_w2**2\n",
    "sigma_b2 = r_b2**2\n",
    "\n",
    "\"\"\"\n",
    "sigma_w0 = tf.log(1+tf.exp(r_w0))\n",
    "sigma_b0 = tf.log(1+tf.exp(r_b0))\n",
    "sigma_w1 = tf.log(1+tf.exp(r_w1))\n",
    "sigma_b1 = tf.log(1+tf.exp(r_b1))\n",
    "\"\"\"\n",
    "\n",
    "y_model = neural_network_test(X,mu_w0,mu_b0,mu_w1,mu_b1,mu_w2,mu_b2)\n",
    "likelihood = tf.tensordot(y_model[0],Y[0],1)\n",
    "cross_ent = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(y_model), reduction_indices=[1]))\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(y_model,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "#entropy(y_model)\n",
    "\n",
    "\n",
    "p=0\n",
    "for i in range(M):\n",
    "    xi = tf.reshape(X[i],[1,D])\n",
    "    yi = tf.reshape(Y[i],[1,D2])\n",
    "    xis = tf.stack([xi]*n_intergal_sample)\n",
    "    yis = tf.stack([yi]*n_intergal_sample)\n",
    "\n",
    "    mu_w0s = tf.stack([mu_w0]*n_intergal_sample)\n",
    "    mu_b0s = tf.stack([mu_b0]*n_intergal_sample)\n",
    "    mu_w1s = tf.stack([mu_w1]*n_intergal_sample)\n",
    "    mu_b1s = tf.stack([mu_b1]*n_intergal_sample)\n",
    "    mu_w2s = tf.stack([mu_w2]*n_intergal_sample)\n",
    "    mu_b2s = tf.stack([mu_b2]*n_intergal_sample)\n",
    "    \n",
    "    sigma_w0s = tf.stack([sigma_w0]*n_intergal_sample)\n",
    "    sigma_b0s = tf.stack([sigma_b0]*n_intergal_sample)\n",
    "    sigma_w1s = tf.stack([sigma_w1]*n_intergal_sample)\n",
    "    sigma_b1s = tf.stack([sigma_b1]*n_intergal_sample)\n",
    "    sigma_w2s = tf.stack([sigma_w2]*n_intergal_sample)\n",
    "    sigma_b2s = tf.stack([sigma_b2]*n_intergal_sample)\n",
    "    \n",
    "       \n",
    "    eps1 = tf.random_normal(shape=[n_intergal_sample,D,h1], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps2 = tf.random_normal(shape=[n_intergal_sample,1,h1], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps3 = tf.random_normal(shape=[n_intergal_sample,h1,h2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps4 = tf.random_normal(shape=[n_intergal_sample,1,h2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps5 = tf.random_normal(shape=[n_intergal_sample,h2,D2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps6 = tf.random_normal(shape=[n_intergal_sample,1,D2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    \n",
    "    w0 = mu_w0s + sigma_w0s * eps1\n",
    "    b0 = mu_b0s + sigma_b0s * eps2\n",
    "    w1 = mu_w1s + sigma_w1s * eps3\n",
    "    b1 = mu_b1s + sigma_b1s * eps4\n",
    "    w2 = mu_w2s + sigma_w2s * eps5\n",
    "    b2 = mu_b2s + sigma_b2s * eps6\n",
    "\n",
    "    y_models = neural_network_test(xis,w0,b0,w1,b1,w2,b2)\n",
    "    pis = tf.reduce_sum(tf.multiply(y_models,yis), 1)\n",
    "    \n",
    "    cross_ents = tf.reduce_mean(-tf.reduce_sum(yis*tf.log(y_models), reduction_indices=[1]))\n",
    "    \n",
    "    pi = tf.reduce_mean(pis)\n",
    "    p = p + tf.log(pi)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(-p)\n",
    "train_step1 = tf.train.GradientDescentOptimizer(0.005).minimize(cross_ent)\n",
    "#train_step1 = tf.train.GradientDescentOptimizer(0.05).minimize(-likelihood)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lm_w0=[]\n",
    "    ls_w0=[]\n",
    "    lm_b0=[]\n",
    "    ls_b0=[]\n",
    "    \n",
    "    lm_w1=[]\n",
    "    ls_w1=[]\n",
    "    lm_b1=[]\n",
    "    ls_b1=[]\n",
    "    \n",
    "    lm_w2=[]\n",
    "    ls_w2=[]\n",
    "    lm_b2=[]\n",
    "    ls_b2=[]\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(100000):\n",
    "        #seed = np.random.randint(0,n,M)\n",
    "        #print x[seed]\n",
    "        #x_batch = x[seed].reshape((M,1))\n",
    "        #y_batch = y[seed].reshape((M,1))\n",
    "        batch = mnist.train.next_batch(M)\n",
    "        x_batch = batch[0]\n",
    "        y_batch = batch[1]\n",
    "        if i%1000==0:\n",
    "            mu1 = sess.run(mu_w0,{X: x_batch, Y: y_batch})\n",
    "            sigma1 = sess.run(sigma_w0,{X: x_batch, Y: y_batch})\n",
    "            mu2 = sess.run(mu_b0,{X: x_batch, Y: y_batch})\n",
    "            sigma2 = sess.run(sigma_b0,{X: x_batch, Y: y_batch})\n",
    "\n",
    "            mu3 = sess.run(mu_w1,{X: x_batch, Y: y_batch})\n",
    "            sigma3 = sess.run(sigma_w1,{X: x_batch, Y: y_batch})\n",
    "            mu4 = sess.run(mu_b1,{X: x_batch, Y: y_batch})\n",
    "            sigma4 = sess.run(sigma_b1,{X: x_batch, Y: y_batch})\n",
    "\n",
    "            mu5 = sess.run(mu_w2,{X: x_batch, Y: y_batch})\n",
    "            sigma5 = sess.run(sigma_w2,{X: x_batch, Y: y_batch})\n",
    "            mu6 = sess.run(mu_b2,{X: x_batch, Y: y_batch})\n",
    "            sigma6 = sess.run(sigma_b2,{X: x_batch, Y: y_batch})\n",
    "\n",
    "            #r = sess.run(p,{X: x_batch, Y: y_batch})\n",
    "            #v = sess.run(var*2,{X: x_batch, Y: y_batch})\n",
    "\n",
    "            accu = sess.run(accuracy,{X: x_batch, Y: y_batch})\n",
    "            test_error = sess.run(accuracy, {X:mnist.test.images, Y:mnist.test.labels})\n",
    "            soft = sess.run(likelihood,{X: x_batch, Y: y_batch})\n",
    "\n",
    "            lm_w0.append(mu1)\n",
    "            ls_w0.append(sigma1)\n",
    "            lm_b0.append(mu2)\n",
    "            ls_b0.append(sigma2)\n",
    "            lm_w1.append(mu3)\n",
    "            ls_w1.append(sigma3)\n",
    "            lm_b1.append(mu4)\n",
    "            ls_b1.append(sigma4)\n",
    "            lm_w2.append(mu5)\n",
    "            ls_w2.append(sigma5)\n",
    "            lm_b2.append(mu6)\n",
    "            ls_b2.append(sigma6)\n",
    "            print \"time:\",i, time.time() - start_time, \"train accuracy\", accu, \"test accuracy\", test_error, soft\n",
    "            #myplot()\n",
    "            #myplot1()\n",
    "        \n",
    "        if i < 1000:\n",
    "            sess.run(train_step,{X: x_batch, Y: y_batch})\n",
    "        else:\n",
    "            sess.run(train_step,{X: x_batch, Y: y_batch})\n",
    "            #sess.run(train_step1,{X: x_batch, Y: y_batch})\n",
    "\n",
    "        #sess.run(train_step2,{X: x_batch, Y: y_batch})\n",
    "            \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"time: \")\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myplot():\n",
    "    t = np.linspace(-10,10, 100).reshape((100,1))\n",
    "    plt.plot(x,y,\"d\",color = \"red\")\n",
    "    for i in range(50):\n",
    "\n",
    "        W_0 = np.random.normal(np.mean(lm_w0[-10:],0),np.mean(ls_w0[-10:],0))\n",
    "        b_0 = np.random.normal(np.mean(lm_b0[-10:],0),np.mean(ls_b0[-10:],0))\n",
    "        W_1 = np.random.normal(np.mean(lm_w1[-10:],0),np.mean(ls_w1[-10:],0))\n",
    "        b_1 = np.random.normal(np.mean(lm_b1[-10:],0),np.mean(ls_b1[-10:],0))\n",
    "        W_2 = np.random.normal(np.mean(lm_w2[-10:],0),np.mean(ls_w2[-10:],0))\n",
    "        b_2 = np.random.normal(np.mean(lm_b2[-10:],0),np.mean(ls_b2[-10:],0))\n",
    "\n",
    "        #yy = neural_network_test1(t, W_0, b_0, W_1, b_1, W_2, b_2) + np.random.normal(0,.2)\n",
    "        yy = np.random.normal(neural_network_test1(t, W_0, b_0, W_1, b_1, W_2, b_2),.2)\n",
    "        plt.plot(t,yy,0.0001)\n",
    "        plt.plot(x,y,\"d\",color = \"red\")\n",
    "    plt.show()\n",
    "myplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 0.894484996796 train accuracy 0.0 test accuracy 0.1084 accuracy for sure 1.0\n",
      "0.0\n",
      "time: 1000 2.16865205765 train accuracy 0.9 test accuracy 0.8513 accuracy for sure 0.9957\n",
      "0.4332\n",
      "time: 2000 3.40011191368 train accuracy 1.0 test accuracy 0.8897 accuracy for sure 0.9852\n",
      "0.684\n",
      "time: 3000 4.63715600967 train accuracy 0.9 test accuracy 0.903 accuracy for sure 0.9794\n",
      "0.7763\n",
      "time: 4000 5.95673203468 train accuracy 0.9 test accuracy 0.9138 accuracy for sure 0.9787\n",
      "0.8252\n",
      "time: 5000 7.21977591515 train accuracy 0.8 test accuracy 0.9203 accuracy for sure 0.9776\n",
      "0.8414\n",
      "time: 6000 8.65268993378 train accuracy 0.9 test accuracy 0.9263 accuracy for sure 0.9789\n",
      "0.8539\n",
      "time: 7000 9.93330907822 train accuracy 1.0 test accuracy 0.9287 accuracy for sure 0.9795\n",
      "0.8645\n",
      "time: 8000 11.2016429901 train accuracy 0.8 test accuracy 0.931 accuracy for sure 0.9772\n",
      "0.8759\n",
      "time: 9000 12.7750580311 train accuracy 0.8 test accuracy 0.9314 accuracy for sure 0.9759\n",
      "0.8854\n",
      "time: 10000 14.3801438808 train accuracy 0.9 test accuracy 0.9378 accuracy for sure 0.9785\n",
      "0.8882\n",
      "time: 11000 16.0338380337 train accuracy 0.9 test accuracy 0.9372 accuracy for sure 0.9773\n",
      "0.8997\n",
      "time: 12000 17.3202359676 train accuracy 0.8 test accuracy 0.9428 accuracy for sure 0.9792\n",
      "0.9012\n",
      "time: 13000 18.8949530125 train accuracy 1.0 test accuracy 0.9445 accuracy for sure 0.9801\n",
      "0.9069\n",
      "time: 14000 20.4281458855 train accuracy 0.9 test accuracy 0.9447 accuracy for sure 0.9789\n",
      "0.9123\n",
      "time: 15000 21.7611858845 train accuracy 1.0 test accuracy 0.9469 accuracy for sure 0.9792\n",
      "0.9127\n",
      "time: 16000 23.0622909069 train accuracy 0.9 test accuracy 0.9495 accuracy for sure 0.9809\n",
      "0.9169\n",
      "time: 17000 24.5066890717 train accuracy 1.0 test accuracy 0.9497 accuracy for sure 0.9801\n",
      "0.9149\n",
      "time: 18000 25.965695858 train accuracy 0.8 test accuracy 0.9521 accuracy for sure 0.9806\n",
      "0.9242\n",
      "time: 19000 27.5460379124 train accuracy 1.0 test accuracy 0.9547 accuracy for sure 0.9823\n",
      "0.925\n",
      "time: 20000 28.8647129536 train accuracy 1.0 test accuracy 0.9567 accuracy for sure 0.9828\n",
      "0.9297\n",
      "time: 21000 30.2298178673 train accuracy 0.9 test accuracy 0.9547 accuracy for sure 0.9804\n",
      "0.9325\n",
      "time: 22000 31.8056309223 train accuracy 1.0 test accuracy 0.9582 accuracy for sure 0.983\n",
      "0.9351\n",
      "time: 23000 33.3163368702 train accuracy 1.0 test accuracy 0.9593 accuracy for sure 0.9822\n",
      "0.9385\n",
      "time: 24000 34.6162509918 train accuracy 1.0 test accuracy 0.9584 accuracy for sure 0.9816\n",
      "0.9397\n",
      "time: 25000 36.0435950756 train accuracy 1.0 test accuracy 0.9595 accuracy for sure 0.982\n",
      "0.9412\n",
      "time: 26000 37.4201898575 train accuracy 1.0 test accuracy 0.9605 accuracy for sure 0.9816\n",
      "0.9432\n",
      "time: 27000 38.7949318886 train accuracy 1.0 test accuracy 0.9616 accuracy for sure 0.9826\n",
      "0.9475\n",
      "time: 28000 40.1997060776 train accuracy 1.0 test accuracy 0.9622 accuracy for sure 0.9838\n",
      "0.9447\n",
      "time: 29000 41.4675638676 train accuracy 1.0 test accuracy 0.9626 accuracy for sure 0.9822\n",
      "0.946\n",
      "time: 30000 42.7497329712 train accuracy 1.0 test accuracy 0.9642 accuracy for sure 0.9831\n",
      "0.9502\n",
      "time: 31000 44.0394098759 train accuracy 1.0 test accuracy 0.9644 accuracy for sure 0.985\n",
      "0.9503\n",
      "time: 32000 45.3133559227 train accuracy 0.8 test accuracy 0.964 accuracy for sure 0.9827\n",
      "0.9542\n",
      "time: 33000 46.702947855 train accuracy 1.0 test accuracy 0.9654 accuracy for sure 0.9823\n",
      "0.9522\n",
      "time: 34000 47.9912700653 train accuracy 1.0 test accuracy 0.9654 accuracy for sure 0.9831\n",
      "0.9554\n",
      "time: 35000 49.2811539173 train accuracy 1.0 test accuracy 0.9655 accuracy for sure 0.9832\n",
      "0.9552\n",
      "time: 36000 50.5558269024 train accuracy 0.9 test accuracy 0.9648 accuracy for sure 0.9831\n",
      "0.9566\n",
      "time: 37000 51.8567988873 train accuracy 0.9 test accuracy 0.9669 accuracy for sure 0.9835\n",
      "0.9585\n",
      "time: 38000 53.1320080757 train accuracy 1.0 test accuracy 0.9688 accuracy for sure 0.9837\n",
      "0.9595\n",
      "time: 39000 54.5137019157 train accuracy 1.0 test accuracy 0.9687 accuracy for sure 0.9844\n",
      "0.9601\n",
      "time: 40000 55.7911229134 train accuracy 1.0 test accuracy 0.9687 accuracy for sure 0.9836\n",
      "0.9631\n",
      "time: 41000 57.0881440639 train accuracy 1.0 test accuracy 0.9684 accuracy for sure 0.9844\n",
      "0.9609\n",
      "time: 42000 58.3429908752 train accuracy 0.9 test accuracy 0.9683 accuracy for sure 0.9827\n",
      "0.9641\n",
      "time: 43000 59.6117880344 train accuracy 1.0 test accuracy 0.9696 accuracy for sure 0.9836\n",
      "0.9652\n",
      "time: 44000 61.0009350777 train accuracy 1.0 test accuracy 0.9688 accuracy for sure 0.9837\n",
      "0.964\n",
      "time: 45000 62.2789270878 train accuracy 1.0 test accuracy 0.9709 accuracy for sure 0.9841\n",
      "0.9628\n",
      "time: 46000 63.5548119545 train accuracy 1.0 test accuracy 0.9711 accuracy for sure 0.985\n",
      "0.9637\n",
      "time: 47000 64.8576059341 train accuracy 0.9 test accuracy 0.971 accuracy for sure 0.9845\n",
      "0.9672\n",
      "time: 48000 66.1606040001 train accuracy 1.0 test accuracy 0.9714 accuracy for sure 0.9853\n",
      "0.9677\n",
      "time: 49000 67.4395780563 train accuracy 1.0 test accuracy 0.9713 accuracy for sure 0.9843\n",
      "0.9691\n",
      "time: 50000 68.8116569519 train accuracy 0.9 test accuracy 0.9729 accuracy for sure 0.9861\n",
      "0.967\n",
      "time: 51000 70.1398420334 train accuracy 1.0 test accuracy 0.9717 accuracy for sure 0.9853\n",
      "0.9686\n",
      "time: 52000 71.5073599815 train accuracy 0.9 test accuracy 0.9718 accuracy for sure 0.9849\n",
      "0.9698\n",
      "time: 53000 72.9039540291 train accuracy 1.0 test accuracy 0.9736 accuracy for sure 0.9855\n",
      "0.9695\n",
      "time: 54000 74.2076730728 train accuracy 1.0 test accuracy 0.9731 accuracy for sure 0.9849\n",
      "0.9711\n",
      "time: 55000 75.7489290237 train accuracy 0.9 test accuracy 0.9734 accuracy for sure 0.9851\n",
      "0.9721\n",
      "time: 56000 77.0628149509 train accuracy 1.0 test accuracy 0.9724 accuracy for sure 0.9855\n",
      "0.9703\n",
      "time: 57000 78.326570034 train accuracy 1.0 test accuracy 0.9731 accuracy for sure 0.9854\n",
      "0.9712\n",
      "time: 58000 79.6039400101 train accuracy 1.0 test accuracy 0.974 accuracy for sure 0.9864\n",
      "0.9708\n",
      "time: 59000 80.8916330338 train accuracy 0.8 test accuracy 0.9726 accuracy for sure 0.9848\n",
      "0.9724\n",
      "time: 60000 82.1666619778 train accuracy 0.8 test accuracy 0.9731 accuracy for sure 0.9845\n",
      "0.9738\n",
      "time: 61000 83.5393650532 train accuracy 0.9 test accuracy 0.9737 accuracy for sure 0.9848\n",
      "0.9739\n",
      "time: 62000 84.8219389915 train accuracy 1.0 test accuracy 0.9736 accuracy for sure 0.9856\n",
      "0.974\n",
      "time: 63000 86.1001598835 train accuracy 1.0 test accuracy 0.9734 accuracy for sure 0.9855\n",
      "0.9745\n",
      "time: 64000 87.3858978748 train accuracy 1.0 test accuracy 0.9747 accuracy for sure 0.9852\n",
      "0.9764\n",
      "time: 65000 88.6700670719 train accuracy 1.0 test accuracy 0.9743 accuracy for sure 0.9862\n",
      "0.9748\n",
      "time: 66000 90.0485069752 train accuracy 1.0 test accuracy 0.9746 accuracy for sure 0.9851\n",
      "0.9758\n",
      "time: 67000 91.321614027 train accuracy 1.0 test accuracy 0.974 accuracy for sure 0.986\n",
      "0.9755\n",
      "time: 68000 92.5714550018 train accuracy 0.8 test accuracy 0.9752 accuracy for sure 0.986\n",
      "0.9769\n",
      "time: 69000 93.8388400078 train accuracy 1.0 test accuracy 0.9745 accuracy for sure 0.9861\n",
      "0.9762\n",
      "time: 70000 95.1291849613 train accuracy 1.0 test accuracy 0.9753 accuracy for sure 0.9863\n",
      "0.9781\n",
      "time: 71000 96.3813700676 train accuracy 1.0 test accuracy 0.976 accuracy for sure 0.9858\n",
      "0.9775\n",
      "time: 72000 97.7736868858 train accuracy 1.0 test accuracy 0.9764 accuracy for sure 0.9865\n",
      "0.9773\n",
      "time: 73000 99.0328879356 train accuracy 1.0 test accuracy 0.975 accuracy for sure 0.9859\n",
      "0.9772\n",
      "time: 74000 100.291198969 train accuracy 1.0 test accuracy 0.975 accuracy for sure 0.9863\n",
      "0.9775\n",
      "time: 75000 101.591731071 train accuracy 1.0 test accuracy 0.9762 accuracy for sure 0.9862\n",
      "0.9794\n",
      "time: 76000 102.885313034 train accuracy 1.0 test accuracy 0.9749 accuracy for sure 0.9853\n",
      "0.9764\n",
      "time: 77000 104.257777929 train accuracy 1.0 test accuracy 0.9748 accuracy for sure 0.9847\n",
      "0.9764\n",
      "time: 78000 105.533259869 train accuracy 1.0 test accuracy 0.975 accuracy for sure 0.9858\n",
      "0.9798\n",
      "time: 79000 106.81697607 train accuracy 0.9 test accuracy 0.975 accuracy for sure 0.9844\n",
      "0.9777\n",
      "time: 80000 108.110586882 train accuracy 1.0 test accuracy 0.9768 accuracy for sure 0.9863\n",
      "0.9796\n",
      "time: 81000 109.382895947 train accuracy 1.0 test accuracy 0.9771 accuracy for sure 0.9862\n",
      "0.9805\n",
      "time: 82000 110.658186913 train accuracy 0.9 test accuracy 0.9771 accuracy for sure 0.9865\n",
      "0.9793\n",
      "time: 83000 112.057070017 train accuracy 1.0 test accuracy 0.976 accuracy for sure 0.9857\n",
      "0.9799\n",
      "time: 84000 113.332670927 train accuracy 1.0 test accuracy 0.9773 accuracy for sure 0.9859\n",
      "0.9811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 85000 114.585763931 train accuracy 1.0 test accuracy 0.9772 accuracy for sure 0.9855\n",
      "0.9817\n",
      "time: 86000 115.849957943 train accuracy 1.0 test accuracy 0.9767 accuracy for sure 0.9857\n",
      "0.9804\n",
      "time: 87000 117.125354052 train accuracy 1.0 test accuracy 0.9773 accuracy for sure 0.9865\n",
      "0.9808\n",
      "time: 88000 118.480700016 train accuracy 1.0 test accuracy 0.9775 accuracy for sure 0.9851\n",
      "0.9829\n",
      "time: 89000 119.778303862 train accuracy 1.0 test accuracy 0.9774 accuracy for sure 0.986\n",
      "0.9823\n",
      "time: 90000 121.044502974 train accuracy 1.0 test accuracy 0.976 accuracy for sure 0.9846\n",
      "0.9815\n",
      "time: 91000 122.300796032 train accuracy 1.0 test accuracy 0.9764 accuracy for sure 0.9851\n",
      "0.9828\n",
      "time: 92000 123.565042973 train accuracy 1.0 test accuracy 0.9788 accuracy for sure 0.9859\n",
      "0.9827\n",
      "time: 93000 124.840938091 train accuracy 1.0 test accuracy 0.9776 accuracy for sure 0.9856\n",
      "0.9829\n",
      "time: 94000 126.256118059 train accuracy 1.0 test accuracy 0.9788 accuracy for sure 0.9866\n",
      "0.9834\n",
      "time: 95000 127.625298977 train accuracy 1.0 test accuracy 0.9773 accuracy for sure 0.9854\n",
      "0.9838\n",
      "time: 96000 129.051055908 train accuracy 0.9 test accuracy 0.9773 accuracy for sure 0.9854\n",
      "0.9827\n",
      "time: 97000 130.330172062 train accuracy 1.0 test accuracy 0.978 accuracy for sure 0.9854\n",
      "0.9833\n",
      "time: 98000 131.826375961 train accuracy 1.0 test accuracy 0.9789 accuracy for sure 0.9859\n",
      "0.983\n",
      "time: 99000 133.298897982 train accuracy 1.0 test accuracy 0.977 accuracy for sure 0.9847\n",
      "0.9851\n",
      "time: \n",
      "134.242907047\n"
     ]
    }
   ],
   "source": [
    "def neural_network_test(x, W_0, b_0, W_1, b_1,W_2, b_2):\n",
    "    h = tf.nn.relu(tf.matmul(x, W_0) + b_0)\n",
    "    h = tf.nn.relu(tf.matmul(h, W_1) + b_1)\n",
    "    h = (tf.matmul(h, W_2) + b_2)\n",
    "    y = tf.nn.softmax(h)\n",
    "    return y\n",
    "\n",
    "M =10\n",
    "D = 28*28\n",
    "D2 = 10\n",
    "n_intergal_sample = 5\n",
    "h1 = 300\n",
    "h2 = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "Y = tf.placeholder(tf.float32, [None,D2], name='Y')\n",
    "X = tf.placeholder(tf.float32, [None,D], name='X')\n",
    "\n",
    "\n",
    "mu_w0 = tf.get_variable(\"w0_mu\",[D,h1])\n",
    "mu_b0 = tf.get_variable(\"b0_mu\",[1,h1])\n",
    "mu_w1 = tf.get_variable(\"w1_mu\",[h1,h2])\n",
    "mu_b1 = tf.get_variable(\"b1_mu\",[1,h2])\n",
    "mu_w2 = tf.get_variable(\"w2_mu\",[h2,D2])\n",
    "mu_b2 = tf.get_variable(\"b2_mu\",[1,D2])\n",
    "\n",
    "r_w0 = tf.get_variable(\"w0_sigma\",[D,h1])\n",
    "r_b0 = tf.get_variable(\"b0_sigma\",[1,h1])\n",
    "r_w1 = tf.get_variable(\"w1_sigma\",[h1,h2])\n",
    "r_b1 = tf.get_variable(\"b1_sigma\",[1,h2])\n",
    "r_w2 = tf.get_variable(\"w2_sigma\",[h2,D2])\n",
    "r_b2 = tf.get_variable(\"b2_sigma\",[1,D2])\n",
    "\n",
    "sigma_w0 = r_w0**2\n",
    "sigma_b0 = r_b0**2\n",
    "sigma_w1 = r_w1**2\n",
    "sigma_b1 = r_b1**2\n",
    "sigma_w2 = r_w2**2\n",
    "sigma_b2 = r_b2**2\n",
    "\n",
    "\"\"\"\n",
    "sigma_w0 = tf.log(1+tf.exp(r_w0))\n",
    "sigma_b0 = tf.log(1+tf.exp(r_b0))\n",
    "sigma_w1 = tf.log(1+tf.exp(r_w1))\n",
    "sigma_b1 = tf.log(1+tf.exp(r_b1))\n",
    "\"\"\"\n",
    "\n",
    "py_model = neural_network_test(X,mu_w0,mu_b0,mu_w1,mu_b1,mu_w2,mu_b2)\n",
    "likelihood = tf.tensordot(py_model[0],Y[0],1)\n",
    "cross_ent = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(py_model), reduction_indices=[1]))\n",
    "\n",
    "pent = -tf.reduce_sum(py_model*tf.log(py_model), reduction_indices=[1])\n",
    "\n",
    "psure = pent<0.8\n",
    "pcorrect_pred = tf.equal(tf.argmax(py_model,1), tf.argmax(Y,1))\n",
    "\n",
    "pcorrect_pred1 = tf.cast(pcorrect_pred, tf.float32)\n",
    "psure1 = tf.cast(psure, tf.float32)\n",
    "\n",
    "pcorrect_pred_for_sure = tf.add(1 - psure1, pcorrect_pred1) >= 1\n",
    "\n",
    "paccuracy = tf.reduce_mean(tf.cast(pcorrect_pred, tf.float32))\n",
    "paccuracy_for_sure = tf.reduce_mean(tf.cast(pcorrect_pred_for_sure, tf.float32))\n",
    "sure_percent = tf.reduce_mean(psure1)\n",
    "\n",
    "test1 = psure\n",
    "test2 = psure1\n",
    "test3 = 1 - psure1\n",
    "\n",
    "p=0\n",
    "c=0\n",
    "ac = 0\n",
    "ac_for_sure = 0\n",
    "for i in range(M):\n",
    "    xi = tf.reshape(X[i],[1,D])\n",
    "    yi = tf.reshape(Y[i],[1,D2])\n",
    "    xis = tf.stack([xi]*n_intergal_sample)\n",
    "    yis = tf.stack([yi]*n_intergal_sample)\n",
    "\n",
    "    mu_w0s = tf.stack([mu_w0]*n_intergal_sample)\n",
    "    mu_b0s = tf.stack([mu_b0]*n_intergal_sample)\n",
    "    mu_w1s = tf.stack([mu_w1]*n_intergal_sample)\n",
    "    mu_b1s = tf.stack([mu_b1]*n_intergal_sample)\n",
    "    mu_w2s = tf.stack([mu_w2]*n_intergal_sample)\n",
    "    mu_b2s = tf.stack([mu_b2]*n_intergal_sample)\n",
    "    \n",
    "    sigma_w0s = tf.stack([sigma_w0]*n_intergal_sample)\n",
    "    sigma_b0s = tf.stack([sigma_b0]*n_intergal_sample)\n",
    "    sigma_w1s = tf.stack([sigma_w1]*n_intergal_sample)\n",
    "    sigma_b1s = tf.stack([sigma_b1]*n_intergal_sample)\n",
    "    sigma_w2s = tf.stack([sigma_w2]*n_intergal_sample)\n",
    "    sigma_b2s = tf.stack([sigma_b2]*n_intergal_sample)\n",
    "    \n",
    "       \n",
    "    eps1 = tf.random_normal(shape=[n_intergal_sample,D,h1], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps2 = tf.random_normal(shape=[n_intergal_sample,1,h1], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps3 = tf.random_normal(shape=[n_intergal_sample,h1,h2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps4 = tf.random_normal(shape=[n_intergal_sample,1,h2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps5 = tf.random_normal(shape=[n_intergal_sample,h2,D2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps6 = tf.random_normal(shape=[n_intergal_sample,1,D2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    \n",
    "    w0 = mu_w0s + sigma_w0s * eps1\n",
    "    b0 = mu_b0s + sigma_b0s * eps2\n",
    "    w1 = mu_w1s + sigma_w1s * eps3\n",
    "    b1 = mu_b1s + sigma_b1s * eps4\n",
    "    w2 = mu_w2s + sigma_w2s * eps5\n",
    "    b2 = mu_b2s + sigma_b2s * eps6\n",
    "\n",
    "    y_models = neural_network_test(xis,w0,b0,w1,b1,w2,b2)\n",
    "    \n",
    "    pis = tf.reduce_sum(tf.multiply(y_models,yis), [1,2])\n",
    "    \"\"\"\n",
    "    #cross_ents = tf.reduce_mean(-tf.reduce_sum(yis*tf.log(y_models), reduction_indices=[1]))\n",
    "    y_model = tf.reduce_mean(y_models,0)\n",
    "    ent = -tf.reduce_sum(y_model*tf.log(y_model), reduction_indices=[1])\n",
    "\n",
    "    sure = ent>0.5\n",
    "    sure1 = tf.cast(sure, tf.float32)\n",
    "    correct_pred = tf.cast(tf.equal(tf.argmax(y_model,1), tf.argmax(Y[i],1)), tf.float32)\n",
    "\n",
    "    correct_pred_for_sure = tf.add(sure1, correct_pred) >= 1\n",
    "    correct_pred_for_sure = tf.cast(correct_pred_for_sure,tf.float32)\n",
    "    \n",
    "    ac+=correct_pred\n",
    "    ac_for_sure+=correct_pred_for_sure\n",
    "    \"\"\"\n",
    "    pi = tf.reduce_mean(pis)\n",
    "    #cross_ent1 = tf.reduce_mean(cross_ents)\n",
    "    #c = c + cross_ent1\n",
    "    p = p + tf.log(pi)\n",
    "    \n",
    "#accuracy = ac/M\n",
    "#accuracy_for_sure = ac_for_sure/M\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(-p)\n",
    "train_step1 = tf.train.GradientDescentOptimizer(0.005).minimize(cross_ent)\n",
    "#train_step1 = tf.train.GradientDescentOptimizer(0.05).minimize(-likelihood)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lm_w0=[]\n",
    "    ls_w0=[]\n",
    "    lm_b0=[]\n",
    "    ls_b0=[]\n",
    "    \n",
    "    lm_w1=[]\n",
    "    ls_w1=[]\n",
    "    lm_b1=[]\n",
    "    ls_b1=[]\n",
    "    \n",
    "    lm_w2=[]\n",
    "    ls_w2=[]\n",
    "    lm_b2=[]\n",
    "    ls_b2=[]\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(100000):\n",
    "        #seed = np.random.randint(0,n,M)\n",
    "        #print x[seed]\n",
    "        #x_batch = x[seed].reshape((M,1))\n",
    "        #y_batch = y[seed].reshape((M,1))\n",
    "        batch = mnist.train.next_batch(M)\n",
    "        x_batch = batch[0]\n",
    "        y_batch = batch[1]\n",
    "        if i%1000==0:\n",
    "            mu1 = sess.run(mu_w0,{X: x_batch, Y: y_batch})\n",
    "            sigma1 = sess.run(sigma_w0,{X: x_batch, Y: y_batch})\n",
    "            mu2 = sess.run(mu_b0,{X: x_batch, Y: y_batch})\n",
    "            sigma2 = sess.run(sigma_b0,{X: x_batch, Y: y_batch})\n",
    "\n",
    "            mu3 = sess.run(mu_w1,{X: x_batch, Y: y_batch})\n",
    "            sigma3 = sess.run(sigma_w1,{X: x_batch, Y: y_batch})\n",
    "            mu4 = sess.run(mu_b1,{X: x_batch, Y: y_batch})\n",
    "            sigma4 = sess.run(sigma_b1,{X: x_batch, Y: y_batch})\n",
    "\n",
    "            mu5 = sess.run(mu_w2,{X: x_batch, Y: y_batch})\n",
    "            sigma5 = sess.run(sigma_w2,{X: x_batch, Y: y_batch})\n",
    "            mu6 = sess.run(mu_b2,{X: x_batch, Y: y_batch})\n",
    "            sigma6 = sess.run(sigma_b2,{X: x_batch, Y: y_batch})\n",
    "\n",
    "            train_error = sess.run(paccuracy,{X: x_batch, Y: y_batch})\n",
    "            test_error = sess.run(paccuracy, {X:mnist.test.images, Y:mnist.test.labels})\n",
    "            accu_for_sure = sess.run(paccuracy_for_sure, {X:mnist.test.images, Y:mnist.test.labels})\n",
    "            t = sess.run(sure_percent, {X:mnist.test.images, Y:mnist.test.labels})\n",
    "            \"\"\"\n",
    "            t1 = sess.run(test1, {X: x_batch, Y: y_batch})\n",
    "            t2 = sess.run(test2, {X: x_batch, Y: y_batch})\n",
    "            t3 = sess.run(test3, {X: x_batch, Y: y_batch})\n",
    "    \n",
    "            soft = sess.run(pent,{X: x_batch, Y: y_batch})\n",
    "            soft1 = sess.run(psure,{X: x_batch, Y: y_batch})\n",
    "            soft2 = sess.run(pcorrect_pred_for_sure,{X: x_batch, Y: y_batch})\n",
    "            \"\"\"\n",
    "            lm_w0.append(mu1)\n",
    "            ls_w0.append(sigma1)\n",
    "            lm_b0.append(mu2)\n",
    "            ls_b0.append(sigma2)\n",
    "            lm_w1.append(mu3)\n",
    "            ls_w1.append(sigma3)\n",
    "            lm_b1.append(mu4)\n",
    "            ls_b1.append(sigma4)\n",
    "            lm_w2.append(mu5)\n",
    "            ls_w2.append(sigma5)\n",
    "            lm_b2.append(mu6)\n",
    "            ls_b2.append(sigma6)\n",
    "            print \"time:\",i, time.time() - start_time, \"train accuracy\", train_error, \"test accuracy\", test_error,\"accuracy for sure\",accu_for_sure\n",
    "            print t\n",
    "            \"\"\"\n",
    "            print t1\n",
    "            print t2\n",
    "            print t3\n",
    "            print soft\n",
    "            print soft1\n",
    "            print soft2\n",
    "            \"\"\"\n",
    "        if i < 1000:\n",
    "            sess.run(train_step1,{X: x_batch, Y: y_batch})\n",
    "        else:\n",
    "            sess.run(train_step1,{X: x_batch, Y: y_batch})\n",
    "            #sess.run(train_step1,{X: x_batch, Y: y_batch})\n",
    "\n",
    "        #sess.run(train_step2,{X: x_batch, Y: y_batch})\n",
    "            \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"time: \")\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  7,  8,  9,  2],\n",
       "       [ 3,  4,  5, 10, 11, 12,  2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,3],[3,4,5]]\n",
    "b = [[7,8,9,2],[10,11,12,2]]\n",
    "np.concatenate([a,b],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./classification_mnist/model.ckpt\n",
      "time: 0 0.7875990867614746\n",
      "-0.029235635\n",
      "train: 1.0 test: 0.9683\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "[[0.00208949 0.01587804 0.00402303 0.01130913 0.09880812 0.00921612\n",
      "  0.00326218 0.24517821 0.00859935 0.6016361 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00193289 0.0208991  0.00458472 0.01593112 0.08598191 0.01317981\n",
      "  0.0032603  0.2719804  0.01097124 0.5712787 ]]\n",
      "0.2382615\n",
      "[[0.00098996 0.01149268 0.0023723  0.00847405 0.07001519 0.00698961\n",
      "  0.00165345 0.26094025 0.00575817 0.63131446]]\n",
      "0.28605235\n",
      "[[0.00189308 0.01824709 0.00426055 0.01212468 0.09025138 0.01043722\n",
      "  0.00312124 0.22344406 0.00862127 0.62759936]]\n",
      "0.2677824\n",
      "[[0.00107088 0.01099041 0.00245138 0.0082609  0.06622978 0.0069561\n",
      "  0.00169366 0.34826833 0.00566794 0.54841053]]\n",
      "0.28376535\n",
      "[[0.00101174 0.01439749 0.00265998 0.00952329 0.06003464 0.0076363\n",
      "  0.0017722  0.3404125  0.00646073 0.55609107]]\n",
      "0.30694312\n",
      "[[0.00163805 0.01647941 0.00362288 0.01060676 0.07276546 0.00893549\n",
      "  0.00270326 0.37203404 0.00783712 0.50337785]]\n",
      "0.33739883\n",
      "[[0.00114768 0.01830558 0.00348629 0.01275064 0.06105387 0.0104571\n",
      "  0.00212636 0.37779027 0.00871377 0.50416845]]\n",
      "0.37945405\n",
      "[[0.00062834 0.01257654 0.00190835 0.00876307 0.07153171 0.00654465\n",
      "  0.00118539 0.39533097 0.00558366 0.49594748]]\n",
      "0.4231315\n",
      "[[0.00167995 0.01629976 0.0036696  0.01056001 0.05911275 0.00874481\n",
      "  0.00277851 0.40124667 0.00764234 0.48826545]]\n",
      "0.390596\n",
      "[[0.00223779 0.01905594 0.00469408 0.01268548 0.05067816 0.01067318\n",
      "  0.00349526 0.392846   0.00898697 0.49464718]]\n",
      "0.43452108\n",
      "[[0.00159556 0.01588134 0.00334047 0.01206401 0.04015487 0.00894765\n",
      "  0.0025266  0.46992713 0.00770615 0.43785617]]\n",
      "0.41478288\n",
      "[[0.002648   0.02324449 0.005437   0.0153976  0.0540562  0.01245318\n",
      "  0.00417825 0.45398593 0.01110898 0.41749024]]\n",
      "0.43378296\n",
      "[[0.0015113  0.0166779  0.00352046 0.01235533 0.04254103 0.00946006\n",
      "  0.00246449 0.47352067 0.00831205 0.4296365 ]]\n",
      "0.4764386\n",
      "[[0.00110097 0.01340532 0.00293841 0.00889743 0.0377281  0.00698273\n",
      "  0.00193503 0.4584251  0.00612455 0.46246234]]\n",
      "0.4872845\n",
      "[[0.00155055 0.01644046 0.00337609 0.01138884 0.04949928 0.00864387\n",
      "  0.00247632 0.48570356 0.00792948 0.41299134]]\n",
      "0.50122285\n",
      "[[0.00147831 0.01393682 0.00332244 0.01071746 0.04605293 0.00833073\n",
      "  0.002262   0.5228527  0.0069011  0.38414538]]\n",
      "0.52547854\n",
      "[[0.00281651 0.02123363 0.00558778 0.01568108 0.04120994 0.01209179\n",
      "  0.00404267 0.5258782  0.01068476 0.36077362]]\n",
      "0.5459523\n",
      "[[0.00079928 0.01554403 0.00246687 0.01077631 0.04605947 0.00796482\n",
      "  0.00133375 0.60320866 0.00649268 0.30535403]]\n",
      "0.5322004\n",
      "[[0.00222921 0.01809615 0.00446476 0.01302688 0.0387282  0.00994043\n",
      "  0.00327092 0.56925017 0.00877474 0.33221847]]\n",
      "0.5350739\n",
      "[[5.8080239e-04 1.1338752e-02 1.7178699e-03 7.5428886e-03 3.8395725e-02\n",
      "  5.8228169e-03 9.2803215e-04 6.3908887e-01 4.5961924e-03 2.8998789e-01]]\n",
      "0.5662091\n",
      "[[0.00080811 0.01459619 0.00263002 0.01103921 0.03303751 0.00795829\n",
      "  0.00142935 0.58308625 0.00685232 0.33856294]]\n",
      "0.5526162\n",
      "[[0.00125743 0.01340477 0.00279366 0.00894151 0.02820695 0.00683778\n",
      "  0.00193723 0.6364255  0.00597633 0.29421893]]\n",
      "0.61572284\n",
      "[[0.00117908 0.01356859 0.00299746 0.01016941 0.02717612 0.00751719\n",
      "  0.00189661 0.64327407 0.00643724 0.2857843 ]]\n",
      "0.6349639\n",
      "[[0.00115351 0.01544511 0.00298878 0.0113726  0.0350477  0.00800918\n",
      "  0.00189385 0.58906746 0.00694666 0.328075  ]]\n",
      "0.5718041\n",
      "[[0.00202426 0.0186625  0.00448277 0.01322376 0.03734512 0.01009588\n",
      "  0.00313067 0.6014818  0.00876185 0.3007915 ]]\n",
      "0.58434945\n",
      "[[0.00141554 0.01438325 0.00313338 0.01110701 0.03196754 0.00819308\n",
      "  0.00207652 0.62972105 0.00674606 0.2912568 ]]\n",
      "0.6241616\n",
      "[[0.00138124 0.01503348 0.00329917 0.01128451 0.03140098 0.00838072\n",
      "  0.00210788 0.63044024 0.00697831 0.28969347]]\n",
      "0.6201664\n",
      "[[0.00138652 0.0135228  0.00294626 0.01051865 0.0322736  0.00738985\n",
      "  0.00196021 0.6583786  0.00652562 0.265098  ]]\n",
      "0.6477174\n",
      "[[0.00172806 0.01851106 0.00408292 0.01430585 0.03764645 0.01033922\n",
      "  0.00265871 0.66052216 0.00889357 0.24131238]]\n",
      "0.6725941\n",
      "[[0.00116573 0.01557478 0.0030623  0.01176684 0.03151527 0.00816302\n",
      "  0.00196156 0.67881906 0.00698096 0.24099055]]\n",
      "0.70518994\n",
      "[[0.0012229  0.01656792 0.00337698 0.01190381 0.03368363 0.00858186\n",
      "  0.00209794 0.656008   0.00750058 0.25905642]]\n",
      "0.6101974\n",
      "[[0.00114389 0.01473676 0.0027062  0.0118295  0.02941954 0.00803785\n",
      "  0.00162504 0.66214734 0.00668719 0.26166654]]\n",
      "0.7352345\n",
      "[[0.0018783  0.01740372 0.00421432 0.01333353 0.02639017 0.00943709\n",
      "  0.00295866 0.6646898  0.0083316  0.25136253]]\n",
      "0.65810335\n",
      "[[0.00162664 0.01630484 0.00376281 0.01192429 0.02924701 0.00845082\n",
      "  0.00257695 0.67988497 0.00732202 0.23889984]]\n",
      "0.6677453\n",
      "[[0.00179863 0.01605688 0.004046   0.01184984 0.02674149 0.00842073\n",
      "  0.00276095 0.7074201  0.0077158  0.21318959]]\n",
      "0.685748\n",
      "[[0.00307164 0.02160553 0.00601436 0.0156871  0.03240457 0.01154743\n",
      "  0.00444768 0.6647388  0.01047971 0.23000297]]\n",
      "0.7153249\n",
      "[[0.00194916 0.01858549 0.00437439 0.01426826 0.03552049 0.00983507\n",
      "  0.00287472 0.68323    0.00883835 0.22052397]]\n",
      "0.68658304\n",
      "[[0.00174285 0.01671176 0.00398247 0.01348075 0.03045621 0.00964254\n",
      "  0.00249299 0.69747275 0.00815083 0.21586676]]\n",
      "0.73597276\n",
      "[[0.00097751 0.013272   0.00243783 0.00914368 0.02665055 0.00600322\n",
      "  0.00150334 0.7315436  0.00536315 0.20310515]]\n",
      "0.6826727\n",
      "[[0.00245218 0.01984539 0.00537914 0.01369454 0.02700813 0.01016633\n",
      "  0.00374408 0.6913705  0.00945411 0.21688542]]\n",
      "0.71118575\n",
      "[[0.00243121 0.02449303 0.00576369 0.01686992 0.03456927 0.01234349\n",
      "  0.0037448  0.70092803 0.0111615  0.18769509]]\n",
      "0.7646005\n",
      "[[0.00150825 0.02170524 0.00444881 0.01581272 0.03535598 0.01099618\n",
      "  0.00261563 0.67779016 0.00983459 0.21993229]]\n",
      "0.69788283\n",
      "[[0.00192245 0.01848361 0.00436316 0.01358379 0.02927043 0.00958828\n",
      "  0.00295131 0.71297187 0.00856552 0.1982996 ]]\n",
      "0.71722186\n",
      "[[0.00175063 0.01820164 0.00421513 0.01360213 0.02954097 0.00956552\n",
      "  0.00279135 0.7297971  0.00873625 0.18179926]]\n",
      "0.72647333\n",
      "[[0.00127204 0.01601273 0.00344327 0.01160921 0.02346851 0.00786975\n",
      "  0.002182   0.7391076  0.00722876 0.18780605]]\n",
      "0.7418332\n",
      "[[0.00164288 0.01666074 0.00382506 0.0124769  0.0273917  0.00886746\n",
      "  0.00263138 0.7222506  0.00766813 0.19658485]]\n",
      "0.7450306\n",
      "[[0.00181824 0.01862109 0.0044653  0.01395721 0.02819906 0.00976748\n",
      "  0.00282154 0.7273352  0.00862163 0.18439315]]\n",
      "0.74817663\n",
      "[[0.00228817 0.01870007 0.00484533 0.0138782  0.02697969 0.01007187\n",
      "  0.0032304  0.7194133  0.00885101 0.19174199]]\n",
      "0.7787009\n",
      "[[6.2484271e-04 1.2393010e-02 2.1848213e-03 9.2486516e-03 2.4829183e-02\n",
      "  6.2507284e-03 1.0249310e-03 7.5126684e-01 5.1234048e-03 1.8705347e-01]]\n",
      "0.7483455\n",
      "[[0.00120249 0.01300468 0.00294998 0.00992631 0.02211668 0.00683709\n",
      "  0.00183813 0.76214457 0.00584893 0.17413104]]\n",
      "0.7295198\n",
      "[[0.00201047 0.02038097 0.0051266  0.01527908 0.03002076 0.01070158\n",
      "  0.0032424  0.71112704 0.00995154 0.19215924]]\n",
      "0.77948004\n",
      "[[0.00170329 0.01778943 0.00415503 0.01356723 0.02795094 0.00943774\n",
      "  0.0025088  0.7372989  0.00796628 0.17762202]]\n",
      "0.74141824\n",
      "[[0.00159866 0.01692105 0.00374208 0.0118415  0.02357238 0.0081985\n",
      "  0.00248419 0.755584   0.00751747 0.16854002]]\n",
      "0.7342545\n",
      "[[0.00159771 0.01761072 0.00389537 0.01213353 0.02836733 0.00853015\n",
      "  0.00252239 0.76332307 0.00761697 0.15440312]]\n",
      "0.76959467\n",
      "[[0.00164966 0.01840846 0.00411145 0.01278855 0.02628708 0.00893107\n",
      "  0.00255637 0.76341456 0.00795209 0.15390079]]\n",
      "0.76401746\n",
      "[[0.00227173 0.01754366 0.00469561 0.01333687 0.02682895 0.00929902\n",
      "  0.00337493 0.74302614 0.00875025 0.17087299]]\n",
      "0.7706349\n",
      "[[0.00134038 0.01441027 0.00328061 0.01200752 0.02652592 0.00792666\n",
      "  0.00215335 0.74245787 0.0069257  0.18297149]]\n",
      "0.7616733\n",
      "[[0.00184308 0.01656645 0.00410418 0.01308429 0.02425097 0.00902777\n",
      "  0.00268688 0.7408411  0.00776416 0.1798309 ]]\n",
      "0.77902603\n",
      "[[0.00123594 0.01640008 0.003365   0.01307986 0.02589539 0.00834813\n",
      "  0.00192834 0.7607401  0.00722385 0.16178314]]\n",
      "0.7563896\n",
      "[[0.00203038 0.01715034 0.00462261 0.01280956 0.02190527 0.00906893\n",
      "  0.00292079 0.760079   0.00818649 0.16122647]]\n",
      "0.8193763\n",
      "[[0.00143404 0.01715255 0.00395282 0.01277604 0.02696731 0.00889755\n",
      "  0.00227011 0.76736784 0.00788173 0.15130007]]\n",
      "0.7781059\n",
      "[[7.2649121e-04 1.3690054e-02 2.6311157e-03 9.7081643e-03 2.2418762e-02\n",
      "  6.3770004e-03 1.3103103e-03 7.8723526e-01 5.5473284e-03 1.5035543e-01]]\n",
      "0.74093103\n",
      "[[0.00154969 0.01438071 0.00365684 0.01133972 0.01924273 0.00772248\n",
      "  0.00237654 0.7912419  0.00681137 0.14167798]]\n",
      "0.780659\n",
      "[[0.00182291 0.01837082 0.00453368 0.01320709 0.02739328 0.00931858\n",
      "  0.00281651 0.73603827 0.00829344 0.1782055 ]]\n",
      "0.7976627\n",
      "[[0.00134606 0.01657828 0.00361105 0.01148779 0.0261628  0.00792352\n",
      "  0.00210106 0.7652084  0.0071005  0.15848057]]\n",
      "0.8193637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00156772 0.01693441 0.0041024  0.01174963 0.02375599 0.00846473\n",
      "  0.00248749 0.78727496 0.00745265 0.13620992]]\n",
      "0.79700905\n",
      "[[0.00147791 0.01340881 0.00327005 0.00987588 0.01976285 0.00668828\n",
      "  0.00216332 0.7858217  0.00610778 0.1514234 ]]\n",
      "0.8115776\n",
      "[[6.7567045e-04 1.2678938e-02 2.2018617e-03 1.0576909e-02 1.9259913e-02\n",
      "  6.4008827e-03 1.1902159e-03 7.7935195e-01 5.5610747e-03 1.6210245e-01]]\n",
      "0.77273047\n",
      "[[0.00177829 0.01636506 0.00425667 0.01237424 0.02178542 0.00885729\n",
      "  0.00258682 0.785781   0.00775141 0.13846396]]\n",
      "0.7979846\n",
      "[[0.00186815 0.01646064 0.00386352 0.01191029 0.02282985 0.00777946\n",
      "  0.00266418 0.7834816  0.00746367 0.14167887]]\n",
      "0.7954834\n",
      "[[0.00187272 0.0172918  0.00442523 0.01245138 0.02545339 0.00889108\n",
      "  0.00287213 0.77823496 0.008097   0.14041047]]\n",
      "0.78405595\n",
      "[[6.8291108e-04 1.1397282e-02 2.0755250e-03 7.4078371e-03 1.8254833e-02\n",
      "  5.0195171e-03 1.1684571e-03 8.4675562e-01 4.3011922e-03 1.0293702e-01]]\n",
      "0.79403776\n",
      "[[0.00143465 0.01473464 0.0034761  0.0113145  0.02134559 0.0072832\n",
      "  0.00230502 0.7976725  0.00672164 0.13371234]]\n",
      "0.80539584\n",
      "[[0.00189044 0.02110916 0.00470454 0.01456975 0.02493828 0.0099785\n",
      "  0.00298802 0.7860939  0.00901171 0.12471569]]\n",
      "0.81267273\n",
      "[[0.00260339 0.02528114 0.00585183 0.01659568 0.0289139  0.01147342\n",
      "  0.00379898 0.7358369  0.01055511 0.15908979]]\n",
      "0.8196936\n",
      "[[0.00180506 0.01512429 0.00384474 0.0112698  0.02335378 0.00769118\n",
      "  0.00255361 0.8022172  0.00706152 0.1250787 ]]\n",
      "0.81567264\n",
      "[[0.00175858 0.01414626 0.00368045 0.01031148 0.02090921 0.00713081\n",
      "  0.00260318 0.81307    0.0065089  0.119881  ]]\n",
      "0.7874249\n",
      "[[0.00191694 0.0203727  0.00502513 0.01462717 0.02701497 0.01036923\n",
      "  0.00303817 0.78585327 0.00884825 0.12293415]]\n",
      "0.82611203\n",
      "[[0.000992   0.01174127 0.00266003 0.00949978 0.01810355 0.00612559\n",
      "  0.00161694 0.8234915  0.00555893 0.12021025]]\n",
      "0.8291972\n",
      "[[0.00209801 0.01566295 0.00442517 0.01101822 0.0212479  0.00812356\n",
      "  0.0031236  0.8232615  0.00738825 0.1036505 ]]\n",
      "0.8263482\n",
      "[[0.00118251 0.017736   0.00337976 0.01180053 0.02572112 0.00772507\n",
      "  0.00191516 0.810831   0.00720755 0.11250141]]\n",
      "0.75352526\n",
      "[[0.00174068 0.01779726 0.00427895 0.01281874 0.02146865 0.00875496\n",
      "  0.00264698 0.8038949  0.00783828 0.11876044]]\n",
      "0.7851566\n",
      "[[0.00122898 0.0133571  0.00302388 0.00914347 0.01750598 0.00602306\n",
      "  0.00196586 0.84403825 0.00539891 0.09831432]]\n",
      "0.8124277\n",
      "[[0.00165312 0.01158049 0.00293494 0.00922595 0.01760263 0.00590632\n",
      "  0.00220523 0.82920337 0.00546072 0.11422747]]\n",
      "0.7844337\n",
      "[[0.00158095 0.01867798 0.00414933 0.01321791 0.02187598 0.00869452\n",
      "  0.00250208 0.8080536  0.00799393 0.11325366]]\n",
      "0.7912376\n",
      "[[0.00136151 0.01634197 0.00375748 0.01162694 0.01946083 0.00764691\n",
      "  0.00228234 0.8152112  0.0070248  0.11528625]]\n",
      "0.8116147\n",
      "[[0.00267336 0.01648379 0.00496562 0.01203709 0.02241023 0.00841842\n",
      "  0.00382459 0.79109037 0.00813339 0.12996286]]\n",
      "0.8201325\n",
      "[[0.00130062 0.01643074 0.00355256 0.01215086 0.02411753 0.00806715\n",
      "  0.00202714 0.8067942  0.00732903 0.1182302 ]]\n",
      "0.8146229\n",
      "[[0.00155775 0.0162816  0.00389718 0.01163231 0.0203361  0.00772933\n",
      "  0.00242136 0.8190964  0.00721419 0.10983369]]\n",
      "0.84573656\n",
      "[[0.00110767 0.01428677 0.00316983 0.01026272 0.0212876  0.00685049\n",
      "  0.0017128  0.8119267  0.00595945 0.12343583]]\n",
      "0.79626304\n",
      "[[0.00129453 0.01848023 0.00374038 0.01281797 0.02489202 0.00820678\n",
      "  0.00212379 0.79534376 0.00782827 0.12527232]]\n",
      "0.8152812\n",
      "[[0.0018037  0.01772291 0.0043114  0.0125052  0.02146976 0.00829169\n",
      "  0.00265056 0.8102816  0.00792869 0.11303425]]\n",
      "0.77300996\n",
      "[[0.00173148 0.01765918 0.00395787 0.0111718  0.01955831 0.00741995\n",
      "  0.00254994 0.826282   0.00718546 0.10248396]]\n",
      "0.8089586\n",
      "[[0.00098926 0.01483293 0.00287237 0.01043382 0.02184603 0.00670334\n",
      "  0.00145906 0.81671196 0.00607318 0.1180779 ]]\n",
      "0.81661636\n",
      "[[0.00155955 0.01805983 0.00417439 0.01290406 0.0237562  0.0086841\n",
      "  0.00243835 0.8023974  0.00807882 0.11794769]]\n",
      "0.79718125\n",
      "[[0.00104337 0.01082198 0.00255758 0.00868171 0.0175541  0.00555833\n",
      "  0.00156484 0.85937667 0.00501739 0.08782379]]\n",
      "0.8194624\n",
      "[[0.00192913 0.01675189 0.00431393 0.01155829 0.02276254 0.00814748\n",
      "  0.00279924 0.817749   0.00770987 0.10627846]]\n",
      "0.84370005\n",
      "[[0.00245123 0.02089766 0.00559636 0.0146785  0.02428808 0.01023613\n",
      "  0.00381371 0.8024009  0.00935443 0.10628302]]\n",
      "0.82107943\n",
      "[[0.00093392 0.01312609 0.00255154 0.00914274 0.02115169 0.00580458\n",
      "  0.00146255 0.82309645 0.00519636 0.11753431]]\n",
      "0.83980405\n",
      "[[0.00242737 0.01904942 0.00476721 0.01323154 0.02326589 0.00876211\n",
      "  0.00327578 0.7895172  0.00793407 0.1277694 ]]\n",
      "0.8268073\n",
      "[[0.00133007 0.01452749 0.00334047 0.01093453 0.02061436 0.00703163\n",
      "  0.00202979 0.83460677 0.00626309 0.09932173]]\n",
      "0.8127887\n",
      "[[0.00214896 0.01316071 0.00371951 0.01039194 0.01754743 0.00671875\n",
      "  0.00280759 0.83013886 0.00629017 0.10707617]]\n",
      "0.8048091\n",
      "[[0.00110522 0.01576663 0.00332732 0.0122543  0.02035009 0.00774557\n",
      "  0.00173353 0.8285125  0.00695174 0.10225289]]\n",
      "0.842144\n",
      "[[0.00168961 0.0185278  0.00438946 0.01346604 0.02386921 0.00901094\n",
      "  0.0025769  0.8073337  0.00794049 0.11119598]]\n",
      "0.8172957\n",
      "[[0.00201944 0.0180637  0.00466308 0.01282109 0.0207717  0.0087651\n",
      "  0.00320781 0.8188471  0.00792518 0.1029157 ]]\n",
      "0.8154213\n",
      "[[0.00188296 0.01487425 0.0042119  0.01028608 0.02296389 0.00749874\n",
      "  0.00277776 0.8268918  0.0068349  0.10177759]]\n",
      "0.8368137\n",
      "[[0.00191541 0.01219564 0.00371385 0.00939051 0.01600689 0.00660615\n",
      "  0.00272819 0.8512705  0.00604957 0.09012349]]\n",
      "0.80436754\n",
      "[[0.00200962 0.0173071  0.00476941 0.0122424  0.02007411 0.00851393\n",
      "  0.00310579 0.83590585 0.00778348 0.08828814]]\n",
      "0.80806124\n",
      "[[0.00170692 0.0180379  0.00448571 0.01283481 0.02115592 0.00859261\n",
      "  0.00275489 0.8107149  0.00812601 0.11159041]]\n",
      "0.8258289\n",
      "[[0.00099882 0.01370786 0.00268664 0.01023818 0.02130065 0.00648753\n",
      "  0.00145554 0.8311362  0.00562923 0.10635908]]\n",
      "0.8187736\n",
      "[[0.00107804 0.01667418 0.00325734 0.01156266 0.02075241 0.00714371\n",
      "  0.00175045 0.81804967 0.00671728 0.1130145 ]]\n",
      "0.801306\n",
      "[[0.00142111 0.01675581 0.00392136 0.01198174 0.02250641 0.00813633\n",
      "  0.00228531 0.82381296 0.0075853  0.10159373]]\n",
      "0.8573773\n",
      "[[0.00093949 0.01502715 0.00319999 0.01202021 0.01963945 0.00770239\n",
      "  0.00158852 0.82824814 0.00670823 0.10492637]]\n",
      "0.8476976\n",
      "[[0.00147105 0.01706047 0.00369094 0.01256201 0.02162506 0.00783641\n",
      "  0.00214938 0.81349874 0.0069303  0.11317556]]\n",
      "0.8232261\n",
      "[[0.00120469 0.01469153 0.00327192 0.01041076 0.01923051 0.00697133\n",
      "  0.00185085 0.8338508  0.00619459 0.102323  ]]\n",
      "0.81850326\n",
      "[[0.00306618 0.01967072 0.00591208 0.01676508 0.02469257 0.01128651\n",
      "  0.00409595 0.78274655 0.01014463 0.1216198 ]]\n",
      "0.86047626\n",
      "[[0.00140866 0.01334794 0.00335648 0.00992342 0.01489498 0.00654326\n",
      "  0.00204178 0.86348456 0.00588982 0.07910934]]\n",
      "0.84895784\n",
      "[[0.00115631 0.01465714 0.00328315 0.00983478 0.02112655 0.00649029\n",
      "  0.0018423  0.8400598  0.00617285 0.09537684]]\n",
      "0.8478569\n",
      "[[0.00239506 0.01911898 0.00471069 0.01321395 0.02317766 0.00879069\n",
      "  0.00332324 0.81903756 0.00835318 0.09787896]]\n",
      "0.82093644\n",
      "[[0.00169995 0.01912788 0.00464676 0.01278272 0.02353483 0.00870219\n",
      "  0.00279474 0.82542527 0.007756   0.09352949]]\n",
      "0.84226984\n",
      "[[0.002181   0.02007771 0.00502036 0.0138921  0.02131283 0.00931378\n",
      "  0.0031559  0.81371766 0.00835984 0.10296872]]\n",
      "0.83908355\n",
      "[[0.00142949 0.01495944 0.00352833 0.01100115 0.01980464 0.00731236\n",
      "  0.00207403 0.85054404 0.00641121 0.08293536]]\n",
      "0.83577186\n",
      "[[0.0013105  0.0172502  0.00364979 0.01226416 0.01876227 0.00782382\n",
      "  0.00203932 0.8388375  0.00694923 0.09111332]]\n",
      "0.82302916\n",
      "[[0.00136213 0.01496432 0.00347031 0.0118118  0.01757699 0.00722626\n",
      "  0.00210783 0.8512217  0.00655444 0.0837042 ]]\n",
      "0.84340405\n",
      "[[0.00200304 0.01656648 0.0043411  0.01191242 0.02062113 0.00829043\n",
      "  0.00284556 0.8345045  0.00761895 0.09129615]]\n",
      "0.83274186\n",
      "[[0.00131272 0.01590368 0.00361194 0.01225458 0.02104406 0.0077241\n",
      "  0.00206925 0.83867943 0.00675029 0.09065032]]\n",
      "0.8546082\n",
      "[[0.0014725  0.01631022 0.0039453  0.01162264 0.01825529 0.00763552\n",
      "  0.00235596 0.8452289  0.0070651  0.08610883]]\n",
      "0.8518956\n",
      "[[0.00138547 0.01332706 0.00326502 0.01018539 0.01718987 0.00662275\n",
      "  0.00200548 0.8425412  0.00603348 0.09744433]]\n",
      "0.7986769\n"
     ]
    }
   ],
   "source": [
    "def neural_network_test(x, W_0, b_0, W_1, b_1,W_2, b_2):\n",
    "    h = tf.nn.relu(tf.matmul(x, W_0) + b_0)\n",
    "    h = tf.nn.relu(tf.matmul(h, W_1) + b_1)\n",
    "    h = (tf.matmul(h, W_2) + b_2)\n",
    "    y = tf.nn.softmax(h)\n",
    "    return y\n",
    "\n",
    "M = 1\n",
    "D = 28*28\n",
    "D2 = 10\n",
    "n_intergal_sample = 200\n",
    "h1 = 300\n",
    "h2 = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "Y = tf.placeholder(tf.float32, [None,D2], name='Y')\n",
    "X = tf.placeholder(tf.float32, [None,D], name='X')\n",
    "\n",
    "\n",
    "mu_w0 = tf.get_variable(\"w0_mu\",[D,h1])\n",
    "mu_b0 = tf.get_variable(\"b0_mu\",[1,h1])\n",
    "mu_w1 = tf.get_variable(\"w1_mu\",[h1,h2])\n",
    "mu_b1 = tf.get_variable(\"b1_mu\",[1,h2])\n",
    "mu_w2 = tf.get_variable(\"w2_mu\",[h2,D2])\n",
    "mu_b2 = tf.get_variable(\"b2_mu\",[1,D2])\n",
    "\n",
    "r_w0 = tf.get_variable(\"w0_sigma\",[D,h1])\n",
    "r_b0 = tf.get_variable(\"b0_sigma\",[1,h1])\n",
    "r_w1 = tf.get_variable(\"w1_sigma\",[h1,h2])\n",
    "r_b1 = tf.get_variable(\"b1_sigma\",[1,h2])\n",
    "r_w2 = tf.get_variable(\"w2_sigma\",[h2,D2])\n",
    "r_b2 = tf.get_variable(\"b2_sigma\",[1,D2])\n",
    "\n",
    "sigma_w0 = r_w0**2\n",
    "sigma_b0 = r_b0**2\n",
    "sigma_w1 = r_w1**2\n",
    "sigma_b1 = r_b1**2\n",
    "sigma_w2 = r_w2**2\n",
    "sigma_b2 = r_b2**2\n",
    "\n",
    "\n",
    "py_model = neural_network_test(X,mu_w0,mu_b0,mu_w1,mu_b1,mu_w2,mu_b2)\n",
    "likelihood = tf.tensordot(py_model[0],Y[0],1)\n",
    "cross_ent = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(py_model), reduction_indices=[1]))\n",
    "\n",
    "pent = -tf.reduce_sum(py_model*tf.log(py_model), reduction_indices=[1])\n",
    "psure = pent<0.1\n",
    "pcorrect_pred = tf.equal(tf.argmax(py_model,1), tf.argmax(Y,1))\n",
    "\n",
    "pcorrect_pred1 = tf.cast(pcorrect_pred, tf.float32)\n",
    "psure1 = tf.cast(psure, tf.float32)\n",
    "\n",
    "pcorrect_pred_for_sure = tf.add(1 - psure1, pcorrect_pred1) >= 1\n",
    "\n",
    "paccuracy = tf.reduce_mean(tf.cast(pcorrect_pred, tf.float32))\n",
    "paccuracy_for_sure = tf.reduce_mean(tf.cast(pcorrect_pred_for_sure, tf.float32))\n",
    "psure_percent = tf.reduce_mean(psure1)\n",
    "\n",
    "test1 = psure\n",
    "test2 = psure1\n",
    "test3 = 1 - psure1\n",
    "\n",
    "p=0\n",
    "c=0\n",
    "ac = 0\n",
    "ac_for_sure = 0\n",
    "\n",
    "for i in range(M):\n",
    "    xi = tf.reshape(X,[M,D])\n",
    "    yi = tf.reshape(Y,[M,D2])\n",
    "    xis = tf.stack([xi]*n_intergal_sample)\n",
    "    yis = tf.stack([yi]*n_intergal_sample)\n",
    "    \n",
    "    \"\"\"\n",
    "    mu_w0s = tf.stack([mu_w0]*n_intergal_sample)\n",
    "    mu_b0s = tf.stack([mu_b0]*n_intergal_sample)\n",
    "    mu_w1s = tf.stack([mu_w1]*n_intergal_sample)\n",
    "    mu_b1s = tf.stack([mu_b1]*n_intergal_sample)\n",
    "    mu_w2s = tf.stack([mu_w2]*n_intergal_sample)\n",
    "    mu_b2s = tf.stack([mu_b2]*n_intergal_sample)\n",
    "    \n",
    "    sigma_w0s = tf.stack([sigma_w0]*n_intergal_sample)\n",
    "    sigma_b0s = tf.stack([sigma_b0]*n_intergal_sample)\n",
    "    sigma_w1s = tf.stack([sigma_w1]*n_intergal_sample)\n",
    "    sigma_b1s = tf.stack([sigma_b1]*n_intergal_sample)\n",
    "    sigma_w2s = tf.stack([sigma_w2]*n_intergal_sample)\n",
    "    sigma_b2s = tf.stack([sigma_b2]*n_intergal_sample)\n",
    "    \"\"\"\n",
    "       \n",
    "    eps1 = tf.random_normal(shape=[n_intergal_sample,D,h1], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps2 = tf.random_normal(shape=[n_intergal_sample,1,h1], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps3 = tf.random_normal(shape=[n_intergal_sample,h1,h2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps4 = tf.random_normal(shape=[n_intergal_sample,1,h2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps5 = tf.random_normal(shape=[n_intergal_sample,h2,D2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps6 = tf.random_normal(shape=[n_intergal_sample,1,D2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    \"\"\"\n",
    "    w0 = mu_w0s + sigma_w0s * eps1 \n",
    "    b0 = mu_b0s + sigma_b0s * eps2\n",
    "    w1 = mu_w1s + sigma_w1s * eps3\n",
    "    b1 = mu_b1s + sigma_b1s * eps4\n",
    "    w2 = mu_w2s + sigma_w2s * eps5\n",
    "    b2 = mu_b2s + sigma_b2s * eps6\n",
    "    \"\"\"\n",
    "    w0 = mu_w0 + sigma_w0 * eps1 \n",
    "    b0 = mu_b0 + sigma_b0 * eps2\n",
    "    w1 = mu_w1 + sigma_w1 * eps3\n",
    "    b1 = mu_b1 + sigma_b1 * eps4\n",
    "    w2 = mu_w2 + sigma_w2 * eps5\n",
    "    b2 = mu_b2 + sigma_b2 * eps6\n",
    "    \n",
    "    \n",
    "    eps11 = tf.reshape(eps1,[n_intergal_sample,D*h1])\n",
    "    eps22 = tf.reshape(eps2,[n_intergal_sample,1*h1])\n",
    "    eps33 = tf.reshape(eps3,[n_intergal_sample,h1*h2])\n",
    "    eps44 = tf.reshape(eps4,[n_intergal_sample,1*h2])\n",
    "    eps55 = tf.reshape(eps5,[n_intergal_sample,h2*D2])\n",
    "    eps66 = tf.reshape(eps6,[n_intergal_sample,1*D2])\n",
    "    \n",
    "    input_latent = tf.concat([eps11,eps22,eps33,eps44,eps55,eps66],1)\n",
    "    \n",
    "    hidden1 = tf.layers.dense(input_latent, 15, activation=tf.nn.relu)\n",
    "    #hidden2 = tf.layers.dense(hidden1, 5, activation=tf.nn.relu)\n",
    "    #hidden3 = tf.layers.dense(hidden2, 30, activation=tf.nn.relu)\n",
    "    \n",
    "    output = tf.layers.dense(hidden1,(D*h1 + 1*h1+ h1*h2 + 1*h2 + h2*D2 + 1*D2), activation=None)\n",
    "      \n",
    "    w0 = tf.reshape(output[:,0:D*h1],[n_intergal_sample,D,h1])\n",
    "    b0 = tf.reshape(output[:,D*h1:D*h1+1*h1],[n_intergal_sample,1,h1])\n",
    "    w1 = tf.reshape(output[:,D*h1+1*h1:D*h1+1*h1+h1*h2],[n_intergal_sample,h1,h2])\n",
    "    b1 = tf.reshape(output[:,D*h1+1*h1+h1*h2:D*h1+1*h1+h1*h2+1*h2],[n_intergal_sample,1,h2])\n",
    "    w2 = tf.reshape(output[:,D*h1+1*h1+h1*h2+1*h2:D*h1+1*h1+h1*h2+1*h2+h2*D2],[n_intergal_sample,h2,D2])\n",
    "    b2 = tf.reshape(output[:,D*h1+1*h1+h1*h2+1*h2+h2*D2:D*h1+1*h1+h1*h2+1*h2+h2*D2+1*D2],[n_intergal_sample,1,D2])\n",
    "    \n",
    "    y_models = neural_network_test(xis,w0,b0,w1,b1,w2,b2)\n",
    "    \n",
    "    XX = tf.reshape([X]*n_intergal_sample,[n_intergal_sample,M,D])\n",
    "    pred = tf.reduce_mean(neural_network_test(XX,w0,b0,w1,b1,w2,b2),0)\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    XX_test = tf.reshape([X]*n_intergal_sample,[n_intergal_sample,10000,D])\n",
    "    pred_test = tf.reduce_mean(neural_network_test(XX_test,w0,b0,w1,b1,w2,b2),0)\n",
    "    correct_pred_test = tf.equal(tf.argmax(pred_test,1), tf.argmax(Y,1))\n",
    "    accuracy_test = tf.reduce_mean(tf.cast(correct_pred_test, tf.float32))\n",
    "    \n",
    "    XX_t1 = tf.reshape([X]*n_intergal_sample,[n_intergal_sample,M,D])\n",
    "    raw = neural_network_test(XX_t1,w0,b0,w1,b1,w2,b2)\n",
    "    pred_t1 = tf.reduce_mean(raw,0)\n",
    "    entropy = -tf.reduce_sum(pred_t1*tf.log(pred_t1), reduction_indices=[1])\n",
    "    \n",
    "    correct_pred_t1 = tf.equal(tf.argmax(pred_t1,1), tf.argmax(Y,1))\n",
    "    accuracy_t1 = tf.reduce_mean(tf.cast(correct_pred_t1, tf.float32))\n",
    "    \n",
    "    \n",
    "    \n",
    "    pis = tf.reduce_sum(tf.multiply(y_models,yis), [1,2])\n",
    "\n",
    "    pi = tf.reduce_mean(pis)\n",
    "    #cross_ent1 = tf.reduce_mean(cross_ents)\n",
    "    #c = c + cross_ent1\n",
    "    p = p + tf.log(pi)\n",
    "    \n",
    "eva1 = evaluation(0.2,X,Y, mu_w0,mu_b0,mu_w1,mu_b1,mu_w2,mu_b2, sigma_w0,sigma_b0,sigma_w1,sigma_b1,sigma_w2,sigma_b2)\n",
    "eva2 = evaluation(0.1,X,Y, mu_w0,mu_b0,mu_w1,mu_b1,mu_w2,mu_b2, sigma_w0,sigma_b0,sigma_w1,sigma_b1,sigma_w2,sigma_b2)\n",
    "eva3 = evaluation(0.05,X,Y, mu_w0,mu_b0,mu_w1,mu_b1,mu_w2,mu_b2, sigma_w0,sigma_b0,sigma_w1,sigma_b1,sigma_w2,sigma_b2)\n",
    "\n",
    "\n",
    "#accuracy = ac/M\n",
    "#accuracy_for_sure = ac_for_sure/M\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.000003).minimize(-p)\n",
    "#train_step1 = tf.train.GradientDescentOptimizer(0.005).minimize(cross_ent)\n",
    "#train_step1 = tf.train.GradientDescentOptimizer(0.05).minimize(-likelihood)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lm_w0=[]\n",
    "    ls_w0=[]\n",
    "    lm_b0=[]\n",
    "    ls_b0=[]\n",
    "    \n",
    "    lm_w1=[]\n",
    "    ls_w1=[]\n",
    "    lm_b1=[]\n",
    "    ls_b1=[]\n",
    "    \n",
    "    lm_w2=[]\n",
    "    ls_w2=[]\n",
    "    lm_b2=[]\n",
    "    ls_b2=[]\n",
    "    \n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #save_path = saver.restore(sess, \"./classification_mnist/model_small.ckpt\")\n",
    "    save_path = saver.restore(sess, \"./classification_mnist/model.ckpt\")\n",
    "\n",
    "    for i in range(60000):\n",
    "        #seed = np.random.randint(0,n,M)\n",
    "        #print x[seed]\n",
    "        #x_batch = x[seed].reshape((M,1))\n",
    "        #y_batch = y[seed].reshape((M,1))\n",
    "        batch = mnist.train.next_batch(M)\n",
    "        x_batch = batch[0]\n",
    "        y_batch = batch[1]\n",
    "\n",
    "        \n",
    "        rightornot = sess.run(correct_pred_t1,{X: x_batch, Y: y_batch})\n",
    "        \n",
    "        it=0\n",
    "        if rightornot[0] == False:\n",
    "            predt1= sess.run(pred_t1,{X: x_batch, Y: y_batch})\n",
    "            predi= sess.run(pi,{X: x_batch, Y: y_batch})\n",
    "            print (y_batch)\n",
    "            print (predt1)\n",
    "            plt.imshow(x_batch.reshape(28,28),cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "            while(predi<=0.99):\n",
    "                sess.run(train_step,{X: x_batch, Y: y_batch})\n",
    "                sess.run(train_step,{X: x_batch, Y: y_batch})\n",
    "                it+=1\n",
    "                pred= sess.run(pred_t1,{X: x_batch, Y: y_batch})\n",
    "                predi= sess.run(pi,{X: x_batch, Y: y_batch})\n",
    "                print (pred)\n",
    "                print(predi)\n",
    "                if it > 500: break\n",
    "\n",
    "        if i%200==0:\n",
    "\n",
    "            \n",
    "            #train_error = sess.run(paccuracy,{X: x_batch, Y: y_batch})\n",
    "            #test_error = sess.run(paccuracy, {X:mnist.test.images, Y:mnist.test.labels})\n",
    "            #accu_for_sure = sess.run(paccuracy_for_sure, {X:mnist.test.images, Y:mnist.test.labels})\n",
    "            \"\"\"\n",
    "            t = sess.run(psure_percent, {X:mnist.test.images, Y:mnist.test.labels})\n",
    "            t1 = sess.run(eva1,{X:mnist.test.images, Y:mnist.test.labels})\n",
    "            t2 = sess.run(eva2,{X:mnist.test.images, Y:mnist.test.labels})\n",
    "            t3 = sess.run(eva3,{X:mnist.test.images, Y:mnist.test.labels})\n",
    "                    raw1 = sess.run(raw,{X:x_test, Y:y_test})\n",
    "        var = np.mean(np.std(raw1,0))\n",
    "        it=0\n",
    "        while(var>0.3):\n",
    "            plt.imshow(x_test.reshape(28,28),cmap='gray')\n",
    "            plt.show()\n",
    "            sess.run(train_step,{X: x_batch, Y: y_batch})\n",
    "            it+=1\n",
    "\n",
    "            if it > 100: break\n",
    "            \"\"\"\n",
    "           \n",
    "            print (\"time:\",i, time.time() - start_time)\n",
    "            t = sess.run(p,{X: x_batch, Y: y_batch})\n",
    "            print (t)\n",
    "            lv=[]\n",
    "            ll=[]\n",
    "            \n",
    "            \n",
    "                        \n",
    "            \"\"\"           \n",
    "            for j in range(5000,6000):\n",
    "                x_test = np.reshape(mnist.test.images[j],[1,D])\n",
    "                y_test = np.reshape(mnist.test.labels[j],[1,D2])\n",
    "                #x_test = np.reshape(X_train_OOD[j],[1,D])\n",
    "                #y_test = np.reshape(y_train_OOD[j],[1,D2])\n",
    "                \n",
    "                #x_new = (x_test+np.reshape(mnist.test.images[j-1],[1,D]))/2\n",
    "                #x_test = x_new\n",
    "                #y_test1 = np.reshape(mnist.test.labels[j-1],[1,D2])\n",
    "                \n",
    "                \n",
    "                rawt1 = sess.run(raw,{X:x_test, Y:y_test})\n",
    "                vart1 = np.mean(np.std(rawt1,0))\n",
    "                ent = sess.run(entropy,{X:x_test, Y:y_test})\n",
    "                rightornot = sess.run(correct_pred_t1,{X:x_test, Y:y_test})\n",
    "\n",
    "                ll.append(rightornot[0])\n",
    "                lv.append(vart1)\n",
    "                    \n",
    "                \n",
    "                predt1= sess.run(pred_t1,{X:x_test, Y:y_test})\n",
    "                predt2= sess.run(pred_t1,{X:x_test, Y:y_test})\n",
    "                predt3=np.mean(rawt1,0)\n",
    "                \n",
    "                if rightornot[0] != True:\n",
    "                    print (j)\n",
    "                    print (\"label:\",np.argmax(y_test))\n",
    "                    #print (\"label:\",np.argmax(y_test1))\n",
    "\n",
    "                    print (\"prediction1:\",predt1,rightornot)\n",
    "                    print (\"prediction1:\",predt2,rightornot)\n",
    "                    print (\"prediction1:\",predt3,rightornot)\n",
    "\n",
    "                    print (ent,vart1,-np.sum(predt2*np.log(predt2)))\n",
    "                    \n",
    "                    \n",
    "                    plt.imshow(x_test.reshape(28,28),cmap='gray')\n",
    "                    plt.show()\n",
    "                    \n",
    "                    for pre in rawt1:\n",
    "                        print (np.argmax(pre))\n",
    "                        print (pre)\n",
    "                    \"\"\"\n",
    "            train_accu = sess.run(accuracy,{X: x_batch, Y: y_batch})\n",
    "            test_accu = sess.run(accuracy_test,{X:mnist.test.images, Y:mnist.test.labels})\n",
    "            print (\"train:\", train_accu, \"test:\", test_accu)\n",
    "            #print \"time:\",i, time.time() - start_time, \"train accuracy\", train_error, \"test accuracy\", test_error,\"accuracy for sure\",accu_for_sure\n",
    "            \"\"\"\n",
    "            print t\n",
    "            print (\"new evaluation bound 0.2\",t1)\n",
    "            print (\"new evaluation bound 0.1\",t2)\n",
    "            print (\"new evaluation bound 0.05\",t3)\n",
    "            \"\"\"\n",
    "        if i < 100:\n",
    "            sess.run(train_step,{X: x_batch, Y: y_batch})\n",
    "            \n",
    "        else:\n",
    "            sess.run(train_step,{X: x_batch, Y: y_batch})\n",
    "            sess.run(train_step,{X: x_batch, Y: y_batch})\n",
    "            \n",
    "        if i%100==0:\n",
    "            save_path = saver.save(sess, \"./classification_mnist/model.ckpt\")\n",
    "            #save_path = saver.save(sess, \"./classification_mnist/model_small.ckpt\")\n",
    "            \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"time: \")\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.5\n",
      "0.093 0.5008234413316269\n",
      "0.094 0.507115439943301\n",
      "0.098 0.5073607720214554\n",
      "0.099 0.5090162912375155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array(ll)\n",
    "area=0\n",
    "var=0\n",
    "for tmp in range(0,100):\n",
    "    y_scores = np.array(lv)<tmp*1./1000\n",
    "    area1 = roc_auc_score(y_true, y_scores)\n",
    "    if area1>area:\n",
    "        area = area1\n",
    "        var = tmp*1./1000\n",
    "        print var, area\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30428428952647935"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scores = np.array(lv)>0.01\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train_OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(bound,X,Y, mu_w0,mu_b0,mu_w1,mu_b1,mu_w2,mu_b2, sigma_w0,sigma_b0,sigma_w1,sigma_b1,sigma_w2,sigma_b2):\n",
    "    #point estimation\n",
    "    py_model = neural_network_test(X,mu_w0,mu_b0,mu_w1,mu_b1,mu_w2,mu_b2)\n",
    "    likelihood = tf.tensordot(py_model[0],Y[0],1)\n",
    "    cross_ent = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(py_model), reduction_indices=[1]))\n",
    "\n",
    "    pent = -tf.reduce_sum(py_model*tf.log(py_model), reduction_indices=[1])\n",
    "    psure = pent < bound\n",
    "    pcorrect_pred = tf.equal(tf.argmax(py_model,1), tf.argmax(Y,1))\n",
    "\n",
    "    pcorrect_pred1 = tf.cast(pcorrect_pred, tf.float32)\n",
    "    psure1 = tf.cast(psure, tf.float32)\n",
    "\n",
    "    pcorrect_pred_for_sure = tf.add(1 - psure1, pcorrect_pred1) >= 1\n",
    "\n",
    "    paccuracy = tf.reduce_mean(tf.cast(pcorrect_pred, tf.float32))\n",
    "    paccuracy_for_sure = tf.reduce_mean(tf.cast(pcorrect_pred_for_sure, tf.float32))\n",
    "    psure_percent = tf.reduce_mean(psure1)\n",
    "    \n",
    "    #density estimation\n",
    "    xis = tf.stack([X]*n_intergal_sample)\n",
    "    yis = tf.stack([Y]*n_intergal_sample)\n",
    "\n",
    "    mu_w0s = tf.stack([mu_w0]*n_intergal_sample)\n",
    "    mu_b0s = tf.stack([mu_b0]*n_intergal_sample)\n",
    "    mu_w1s = tf.stack([mu_w1]*n_intergal_sample)\n",
    "    mu_b1s = tf.stack([mu_b1]*n_intergal_sample)\n",
    "    mu_w2s = tf.stack([mu_w2]*n_intergal_sample)\n",
    "    mu_b2s = tf.stack([mu_b2]*n_intergal_sample)\n",
    "    \n",
    "    sigma_w0s = tf.stack([sigma_w0]*n_intergal_sample)\n",
    "    sigma_b0s = tf.stack([sigma_b0]*n_intergal_sample)\n",
    "    sigma_w1s = tf.stack([sigma_w1]*n_intergal_sample)\n",
    "    sigma_b1s = tf.stack([sigma_b1]*n_intergal_sample)\n",
    "    sigma_w2s = tf.stack([sigma_w2]*n_intergal_sample)\n",
    "    sigma_b2s = tf.stack([sigma_b2]*n_intergal_sample)\n",
    "    \n",
    "       \n",
    "    eps1 = tf.random_normal(shape=[n_intergal_sample,D,h1], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps2 = tf.random_normal(shape=[n_intergal_sample,1,h1], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps3 = tf.random_normal(shape=[n_intergal_sample,h1,h2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps4 = tf.random_normal(shape=[n_intergal_sample,1,h2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps5 = tf.random_normal(shape=[n_intergal_sample,h2,D2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    eps6 = tf.random_normal(shape=[n_intergal_sample,1,D2], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "    \n",
    "    w0 = mu_w0s + sigma_w0s * eps1\n",
    "    b0 = mu_b0s + sigma_b0s * eps2\n",
    "    w1 = mu_w1s + sigma_w1s * eps3\n",
    "    b1 = mu_b1s + sigma_b1s * eps4\n",
    "    w2 = mu_w2s + sigma_w2s * eps5\n",
    "    b2 = mu_b2s + sigma_b2s * eps6\n",
    "\n",
    "    y_models = neural_network_test(xis,w0,b0,w1,b1,w2,b2)\n",
    "\n",
    "    y_model_ave = tf.reduce_mean(y_models,[0])\n",
    "    \n",
    "    entropy = -tf.reduce_sum(y_model_ave*tf.log(y_model_ave), reduction_indices=[1])\n",
    "\n",
    "    sure = entropy < bound\n",
    "    correct_predict = tf.equal(tf.argmax(y_model_ave,1), tf.argmax(Y,1))\n",
    "\n",
    "    correct_predict1 = tf.cast(correct_predict, tf.float32)\n",
    "    sure1 = tf.cast(sure, tf.float32)\n",
    "\n",
    "    correct_pred_for_sure = tf.add(1 - sure1, correct_predict1) >= 1\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "    accuracy_for_sure = tf.reduce_mean(tf.cast(correct_pred_for_sure, tf.float32))\n",
    "    sure_percent = tf.reduce_mean(sure1)\n",
    "    return [paccuracy,paccuracy_for_sure,psure_percent,accuracy,accuracy_for_sure,sure_percent,y_model_ave[0:3],entropy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6179"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(my_X[1].reshape(28,28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_X = []\n",
    "def find_you():\n",
    "    for i in range(len(mnist.train.labels)):\n",
    "        if np.argmax(mnist.train.labels[i]) == 1:\n",
    "            my_X.append(mnist.train.images[i])\n",
    "find_you()\n",
    "my_X = np.array(my_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "randlabels = np.random.randint(1, len(my_X),len(my_X)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Y = []\n",
    "for i in range(len(my_X)):\n",
    "    my_Y.append([1.,0.])\n",
    "my_Y = np.array(my_Y)\n",
    "for i in randlabels:\n",
    "    my_Y[i] = [0.,1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.mean(np.mean(ls_w0,1),1))\n",
    "plt.plot(np.mean(np.mean(ls_b1,1),1))\n",
    "plt.plot(np.mean(np.mean(ls_b2,1),1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.9714448e-08"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ls_w0[-1]-ls_w0[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
